{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.0.0\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: c:\\users\\m.amintoosi\\.conda\\envs\\pydml\\lib\\site-packages\n",
      "Requires: filelock, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: ISLP, pytorch-lightning, torch-directml, torch-geometric-temporal, torchaudio, torchmetrics, torchvision\n"
     ]
    }
   ],
   "source": [
    "# !pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cplex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In university run in pydml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading config...\n",
      "Loading dataset...\n",
      "Initializing weights...\n",
      "Get teacher model\n",
      "In channels: 1024\n",
      "Hidden channels: 128\n",
      "Out channels: 2\n",
      "Reseting model parameters...\n",
      "Get optimizer\n",
      "Learning rate: 0.001\n",
      "Weight decay: 0.0005\n",
      "Start training...\n",
      "Epoch: [1/500] loss_train: 0.7995 acc_train: 0.4221 acc_val: 0.5137\n",
      "Acc_best is updated to 0.5137. Model checkpoint is saved to ./model/BA1k_MVC_Teacher.pt.\n",
      "Test accuracy is 0.5470\n",
      "Epoch: [2/500] loss_train: 0.6898 acc_train: 0.5799 acc_val: 0.5137\n",
      "Epoch: [3/500] loss_train: 0.6878 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [4/500] loss_train: 0.6860 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [5/500] loss_train: 0.6742 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [6/500] loss_train: 0.6787 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [7/500] loss_train: 0.6697 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [8/500] loss_train: 0.6679 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [9/500] loss_train: 0.6639 acc_train: 0.5840 acc_val: 0.5137\n",
      "Epoch: [10/500] loss_train: 0.6605 acc_train: 0.5902 acc_val: 0.5137\n",
      "Epoch: [11/500] loss_train: 0.6632 acc_train: 0.5943 acc_val: 0.5137\n",
      "Epoch: [12/500] loss_train: 0.6643 acc_train: 0.6045 acc_val: 0.5137\n",
      "Epoch: [13/500] loss_train: 0.6650 acc_train: 0.6025 acc_val: 0.5137\n",
      "Epoch: [14/500] loss_train: 0.6659 acc_train: 0.6168 acc_val: 0.5137\n",
      "Epoch: [15/500] loss_train: 0.6621 acc_train: 0.6270 acc_val: 0.5195\n",
      "Acc_best is updated to 0.5195. Model checkpoint is saved to ./model/BA1k_MVC_Teacher.pt.\n",
      "Test accuracy is 0.5520\n",
      "Epoch: [16/500] loss_train: 0.6643 acc_train: 0.6537 acc_val: 0.5332\n",
      "Acc_best is updated to 0.5332. Model checkpoint is saved to ./model/BA1k_MVC_Teacher.pt.\n",
      "Test accuracy is 0.5680\n",
      "Epoch: [17/500] loss_train: 0.6595 acc_train: 0.6496 acc_val: 0.5391\n",
      "Acc_best is updated to 0.5391. Model checkpoint is saved to ./model/BA1k_MVC_Teacher.pt.\n",
      "Test accuracy is 0.5760\n",
      "Epoch: [18/500] loss_train: 0.6576 acc_train: 0.6803 acc_val: 0.5371\n",
      "Epoch: [19/500] loss_train: 0.6554 acc_train: 0.6352 acc_val: 0.5293\n",
      "Epoch: [20/500] loss_train: 0.6495 acc_train: 0.6639 acc_val: 0.5156\n",
      "Epoch: [21/500] loss_train: 0.6523 acc_train: 0.6557 acc_val: 0.5137\n",
      "Epoch: [22/500] loss_train: 0.6434 acc_train: 0.6352 acc_val: 0.5137\n",
      "Epoch: [23/500] loss_train: 0.6410 acc_train: 0.6168 acc_val: 0.5137\n",
      "Epoch: [24/500] loss_train: 0.6374 acc_train: 0.5984 acc_val: 0.5137\n",
      "Epoch: [25/500] loss_train: 0.6484 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [26/500] loss_train: 0.6436 acc_train: 0.5922 acc_val: 0.5137\n",
      "Epoch: [27/500] loss_train: 0.6405 acc_train: 0.5840 acc_val: 0.5137\n",
      "Epoch: [28/500] loss_train: 0.6531 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [29/500] loss_train: 0.6429 acc_train: 0.5840 acc_val: 0.5137\n",
      "Epoch: [30/500] loss_train: 0.6510 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [31/500] loss_train: 0.6565 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [32/500] loss_train: 0.6458 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [33/500] loss_train: 0.6454 acc_train: 0.5861 acc_val: 0.5137\n",
      "Epoch: [34/500] loss_train: 0.6418 acc_train: 0.5840 acc_val: 0.5137\n",
      "Epoch: [35/500] loss_train: 0.6360 acc_train: 0.5840 acc_val: 0.5137\n",
      "Epoch: [36/500] loss_train: 0.6288 acc_train: 0.5881 acc_val: 0.5137\n",
      "Epoch: [37/500] loss_train: 0.6143 acc_train: 0.6270 acc_val: 0.5332\n",
      "Epoch: [38/500] loss_train: 0.6158 acc_train: 0.6270 acc_val: 0.6367\n",
      "Acc_best is updated to 0.6367. Model checkpoint is saved to ./model/BA1k_MVC_Teacher.pt.\n",
      "Test accuracy is 0.6640\n",
      "Epoch: [39/500] loss_train: 0.6068 acc_train: 0.6537 acc_val: 0.7383\n",
      "Acc_best is updated to 0.7383. Model checkpoint is saved to ./model/BA1k_MVC_Teacher.pt.\n",
      "Test accuracy is 0.7320\n",
      "Epoch: [40/500] loss_train: 0.5886 acc_train: 0.7172 acc_val: 0.7871\n",
      "Acc_best is updated to 0.7871. Model checkpoint is saved to ./model/BA1k_MVC_Teacher.pt.\n",
      "Test accuracy is 0.7680\n",
      "Epoch: [41/500] loss_train: 0.5912 acc_train: 0.7275 acc_val: 0.8066\n",
      "Acc_best is updated to 0.8066. Model checkpoint is saved to ./model/BA1k_MVC_Teacher.pt.\n",
      "Test accuracy is 0.7820\n",
      "Epoch: [42/500] loss_train: 0.5980 acc_train: 0.7131 acc_val: 0.7910\n",
      "Epoch: [43/500] loss_train: 0.6091 acc_train: 0.7172 acc_val: 0.7266\n",
      "Epoch: [44/500] loss_train: 0.6232 acc_train: 0.7070 acc_val: 0.6758\n",
      "Epoch: [45/500] loss_train: 0.6491 acc_train: 0.6455 acc_val: 0.6230\n",
      "Epoch: [46/500] loss_train: 0.6708 acc_train: 0.5553 acc_val: 0.5742\n",
      "Epoch: [47/500] loss_train: 0.6981 acc_train: 0.5123 acc_val: 0.5371\n",
      "Epoch: [48/500] loss_train: 0.7180 acc_train: 0.4693 acc_val: 0.5098\n",
      "Epoch: [49/500] loss_train: 0.7287 acc_train: 0.4549 acc_val: 0.5039\n",
      "Epoch: [50/500] loss_train: 0.7401 acc_train: 0.4529 acc_val: 0.5078\n",
      "Epoch: [51/500] loss_train: 0.7355 acc_train: 0.4631 acc_val: 0.5156\n",
      "Epoch: [52/500] loss_train: 0.7386 acc_train: 0.4590 acc_val: 0.5469\n",
      "Epoch: [53/500] loss_train: 0.7144 acc_train: 0.4918 acc_val: 0.5781\n",
      "Epoch: [54/500] loss_train: 0.6987 acc_train: 0.5225 acc_val: 0.6172\n",
      "Epoch: [55/500] loss_train: 0.6873 acc_train: 0.5574 acc_val: 0.6602\n",
      "Epoch: [56/500] loss_train: 0.6558 acc_train: 0.6066 acc_val: 0.7012\n",
      "Epoch: [57/500] loss_train: 0.6215 acc_train: 0.6619 acc_val: 0.7461\n",
      "Epoch: [58/500] loss_train: 0.6060 acc_train: 0.7172 acc_val: 0.7930\n",
      "Epoch: [59/500] loss_train: 0.5875 acc_train: 0.7234 acc_val: 0.7969\n",
      "Epoch: [60/500] loss_train: 0.5822 acc_train: 0.7316 acc_val: 0.7949\n",
      "Epoch: [61/500] loss_train: 0.5607 acc_train: 0.7705 acc_val: 0.7812\n",
      "Epoch: [62/500] loss_train: 0.5614 acc_train: 0.7295 acc_val: 0.7500\n",
      "Epoch: [63/500] loss_train: 0.5566 acc_train: 0.7254 acc_val: 0.7305\n",
      "Epoch: [64/500] loss_train: 0.5720 acc_train: 0.7049 acc_val: 0.6777\n",
      "Epoch: [65/500] loss_train: 0.5872 acc_train: 0.6701 acc_val: 0.5703\n",
      "Epoch: [66/500] loss_train: 0.5935 acc_train: 0.6516 acc_val: 0.5293\n",
      "Epoch: [67/500] loss_train: 0.6227 acc_train: 0.6250 acc_val: 0.5137\n",
      "Epoch: [68/500] loss_train: 0.6302 acc_train: 0.6086 acc_val: 0.5137\n",
      "Epoch: [69/500] loss_train: 0.6530 acc_train: 0.5922 acc_val: 0.5137\n",
      "Epoch: [70/500] loss_train: 0.6605 acc_train: 0.5881 acc_val: 0.5137\n",
      "Epoch: [71/500] loss_train: 0.6689 acc_train: 0.5861 acc_val: 0.5137\n",
      "Epoch: [72/500] loss_train: 0.6878 acc_train: 0.5861 acc_val: 0.5137\n",
      "Epoch: [73/500] loss_train: 0.6925 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [74/500] loss_train: 0.7025 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [75/500] loss_train: 0.6852 acc_train: 0.5902 acc_val: 0.5137\n",
      "Epoch: [76/500] loss_train: 0.7102 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [77/500] loss_train: 0.7161 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [78/500] loss_train: 0.7182 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [79/500] loss_train: 0.7184 acc_train: 0.5840 acc_val: 0.5137\n",
      "Epoch: [80/500] loss_train: 0.7276 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [81/500] loss_train: 0.7212 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [82/500] loss_train: 0.7282 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [83/500] loss_train: 0.7238 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [84/500] loss_train: 0.7230 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [85/500] loss_train: 0.7179 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [86/500] loss_train: 0.7270 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [87/500] loss_train: 0.7153 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [88/500] loss_train: 0.7219 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [89/500] loss_train: 0.7135 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [90/500] loss_train: 0.7081 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [91/500] loss_train: 0.7087 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [92/500] loss_train: 0.7063 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [93/500] loss_train: 0.7013 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [94/500] loss_train: 0.7005 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [95/500] loss_train: 0.7012 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [96/500] loss_train: 0.6959 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [97/500] loss_train: 0.6891 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [98/500] loss_train: 0.6805 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [99/500] loss_train: 0.6771 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [100/500] loss_train: 0.6783 acc_train: 0.5820 acc_val: 0.5137\n",
      "Epoch: [101/500] loss_train: 0.6677 acc_train: 0.5840 acc_val: 0.5137\n",
      "Epoch: [102/500] loss_train: 0.6658 acc_train: 0.5840 acc_val: 0.5137\n",
      "Epoch: [103/500] loss_train: 0.6706 acc_train: 0.5799 acc_val: 0.5137\n",
      "Epoch: [104/500] loss_train: 0.6604 acc_train: 0.5840 acc_val: 0.5137\n",
      "Epoch: [105/500] loss_train: 0.6579 acc_train: 0.5861 acc_val: 0.5137\n",
      "Epoch: [106/500] loss_train: 0.6524 acc_train: 0.5943 acc_val: 0.5137\n",
      "Epoch: [107/500] loss_train: 0.6556 acc_train: 0.5799 acc_val: 0.5137\n",
      "Epoch: [108/500] loss_train: 0.6356 acc_train: 0.5943 acc_val: 0.5137\n",
      "Epoch: [109/500] loss_train: 0.6394 acc_train: 0.6025 acc_val: 0.5137\n",
      "Epoch: [110/500] loss_train: 0.6367 acc_train: 0.5861 acc_val: 0.5137\n",
      "Epoch: [111/500] loss_train: 0.6332 acc_train: 0.5861 acc_val: 0.5137\n",
      "Epoch: [112/500] loss_train: 0.6391 acc_train: 0.5922 acc_val: 0.5137\n",
      "Epoch: [113/500] loss_train: 0.6223 acc_train: 0.5984 acc_val: 0.5137\n",
      "Epoch: [114/500] loss_train: 0.6256 acc_train: 0.6045 acc_val: 0.5137\n",
      "Epoch: [115/500] loss_train: 0.6266 acc_train: 0.6066 acc_val: 0.5137\n",
      "Epoch: [116/500] loss_train: 0.6250 acc_train: 0.6004 acc_val: 0.5137\n",
      "Epoch: [117/500] loss_train: 0.6125 acc_train: 0.6168 acc_val: 0.5137\n",
      "Epoch: [118/500] loss_train: 0.6069 acc_train: 0.6250 acc_val: 0.5176\n",
      "Epoch: [119/500] loss_train: 0.6150 acc_train: 0.6189 acc_val: 0.5312\n",
      "Epoch: [120/500] loss_train: 0.6019 acc_train: 0.6475 acc_val: 0.5430\n",
      "Epoch: [121/500] loss_train: 0.6080 acc_train: 0.6475 acc_val: 0.5684\n",
      "Epoch: [122/500] loss_train: 0.6043 acc_train: 0.6496 acc_val: 0.6211\n",
      "Epoch: [123/500] loss_train: 0.6087 acc_train: 0.6578 acc_val: 0.6758\n",
      "Epoch: [124/500] loss_train: 0.6099 acc_train: 0.6270 acc_val: 0.7070\n",
      "Epoch: [125/500] loss_train: 0.6032 acc_train: 0.6824 acc_val: 0.7207\n",
      "Epoch: [126/500] loss_train: 0.6134 acc_train: 0.6639 acc_val: 0.7363\n",
      "Epoch: [127/500] loss_train: 0.6081 acc_train: 0.6557 acc_val: 0.7480\n",
      "Epoch: [128/500] loss_train: 0.5999 acc_train: 0.6967 acc_val: 0.7520\n",
      "Epoch: [129/500] loss_train: 0.5986 acc_train: 0.6967 acc_val: 0.7422\n",
      "Epoch: [130/500] loss_train: 0.5909 acc_train: 0.7131 acc_val: 0.7402\n",
      "Epoch: [131/500] loss_train: 0.6008 acc_train: 0.7008 acc_val: 0.7422\n",
      "Epoch: [132/500] loss_train: 0.5996 acc_train: 0.7090 acc_val: 0.7402\n",
      "Epoch: [133/500] loss_train: 0.5969 acc_train: 0.7111 acc_val: 0.7402\n",
      "Epoch: [134/500] loss_train: 0.6010 acc_train: 0.7131 acc_val: 0.7539\n",
      "Epoch: [135/500] loss_train: 0.6006 acc_train: 0.7193 acc_val: 0.7637\n",
      "Epoch: [136/500] loss_train: 0.5974 acc_train: 0.7418 acc_val: 0.7676\n",
      "Epoch: [137/500] loss_train: 0.5992 acc_train: 0.7336 acc_val: 0.7793\n",
      "Epoch: [138/500] loss_train: 0.6066 acc_train: 0.7111 acc_val: 0.7871\n",
      "Epoch: [139/500] loss_train: 0.5975 acc_train: 0.7439 acc_val: 0.7852\n",
      "Epoch: [140/500] loss_train: 0.6009 acc_train: 0.7357 acc_val: 0.7891\n",
      "Epoch: [141/500] loss_train: 0.6020 acc_train: 0.7480 acc_val: 0.7949\n",
      "Epoch: [142/500] loss_train: 0.6037 acc_train: 0.7275 acc_val: 0.7891\n",
      "Epoch: [143/500] loss_train: 0.6044 acc_train: 0.7459 acc_val: 0.7832\n",
      "Epoch: [144/500] loss_train: 0.6131 acc_train: 0.7295 acc_val: 0.7812\n",
      "Epoch: [145/500] loss_train: 0.6105 acc_train: 0.7316 acc_val: 0.7852\n",
      "Epoch: [146/500] loss_train: 0.6118 acc_train: 0.7377 acc_val: 0.7852\n",
      "Epoch: [147/500] loss_train: 0.6110 acc_train: 0.7398 acc_val: 0.7852\n",
      "Epoch: [148/500] loss_train: 0.6102 acc_train: 0.7418 acc_val: 0.7852\n",
      "Epoch: [149/500] loss_train: 0.6043 acc_train: 0.7541 acc_val: 0.7871\n",
      "Epoch: [150/500] loss_train: 0.6189 acc_train: 0.7316 acc_val: 0.7949\n",
      "Epoch: [151/500] loss_train: 0.6133 acc_train: 0.7357 acc_val: 0.7910\n",
      "Epoch: [152/500] loss_train: 0.6204 acc_train: 0.7357 acc_val: 0.7832\n",
      "Epoch: [153/500] loss_train: 0.6134 acc_train: 0.7398 acc_val: 0.7812\n",
      "Epoch: [154/500] loss_train: 0.6241 acc_train: 0.7275 acc_val: 0.7656\n",
      "Epoch: [155/500] loss_train: 0.6239 acc_train: 0.7152 acc_val: 0.7617\n",
      "Epoch: [156/500] loss_train: 0.6250 acc_train: 0.7234 acc_val: 0.7578\n",
      "Epoch: [157/500] loss_train: 0.6208 acc_train: 0.7439 acc_val: 0.7539\n",
      "Epoch: [158/500] loss_train: 0.6248 acc_train: 0.7070 acc_val: 0.7520\n",
      "Epoch: [159/500] loss_train: 0.6263 acc_train: 0.7111 acc_val: 0.7461\n",
      "Epoch: [160/500] loss_train: 0.6239 acc_train: 0.7336 acc_val: 0.7383\n",
      "Epoch: [161/500] loss_train: 0.6296 acc_train: 0.7049 acc_val: 0.7383\n",
      "Epoch: [162/500] loss_train: 0.6314 acc_train: 0.7049 acc_val: 0.7344\n",
      "Epoch: [163/500] loss_train: 0.6298 acc_train: 0.7193 acc_val: 0.7324\n",
      "Epoch: [164/500] loss_train: 0.6296 acc_train: 0.6988 acc_val: 0.7266\n",
      "Epoch: [165/500] loss_train: 0.6373 acc_train: 0.6947 acc_val: 0.7207\n",
      "Epoch: [166/500] loss_train: 0.6301 acc_train: 0.7295 acc_val: 0.7207\n",
      "Epoch: [167/500] loss_train: 0.6320 acc_train: 0.6926 acc_val: 0.7188\n",
      "Epoch: [168/500] loss_train: 0.6344 acc_train: 0.7029 acc_val: 0.7148\n",
      "Epoch: [169/500] loss_train: 0.6350 acc_train: 0.6988 acc_val: 0.7109\n",
      "Epoch: [170/500] loss_train: 0.6350 acc_train: 0.6865 acc_val: 0.7090\n",
      "Epoch: [171/500] loss_train: 0.6373 acc_train: 0.6844 acc_val: 0.7070\n",
      "Epoch: [172/500] loss_train: 0.6353 acc_train: 0.6824 acc_val: 0.7031\n",
      "Epoch: [173/500] loss_train: 0.6347 acc_train: 0.6885 acc_val: 0.7031\n",
      "Epoch: [174/500] loss_train: 0.6407 acc_train: 0.6557 acc_val: 0.7012\n",
      "Epoch: [175/500] loss_train: 0.6352 acc_train: 0.7049 acc_val: 0.7012\n",
      "Epoch: [176/500] loss_train: 0.6348 acc_train: 0.6844 acc_val: 0.7012\n",
      "Epoch: [177/500] loss_train: 0.6393 acc_train: 0.6803 acc_val: 0.7012\n",
      "Epoch: [178/500] loss_train: 0.6423 acc_train: 0.6803 acc_val: 0.7012\n",
      "Epoch: [179/500] loss_train: 0.6369 acc_train: 0.6803 acc_val: 0.7012\n",
      "Epoch: [180/500] loss_train: 0.6453 acc_train: 0.6537 acc_val: 0.7031\n",
      "Epoch: [181/500] loss_train: 0.6389 acc_train: 0.6824 acc_val: 0.7051\n",
      "Epoch: [182/500] loss_train: 0.6393 acc_train: 0.6721 acc_val: 0.7051\n",
      "Epoch: [183/500] loss_train: 0.6318 acc_train: 0.7008 acc_val: 0.7109\n",
      "Epoch: [184/500] loss_train: 0.6373 acc_train: 0.6865 acc_val: 0.7109\n",
      "Epoch: [185/500] loss_train: 0.6404 acc_train: 0.6762 acc_val: 0.7168\n",
      "Epoch: [186/500] loss_train: 0.6403 acc_train: 0.6885 acc_val: 0.7168\n",
      "Epoch: [187/500] loss_train: 0.6359 acc_train: 0.6967 acc_val: 0.7188\n",
      "Epoch: [188/500] loss_train: 0.6301 acc_train: 0.6967 acc_val: 0.7207\n",
      "Epoch: [189/500] loss_train: 0.6305 acc_train: 0.7070 acc_val: 0.7207\n",
      "Epoch: [190/500] loss_train: 0.6320 acc_train: 0.7131 acc_val: 0.7266\n",
      "Epoch: [191/500] loss_train: 0.6311 acc_train: 0.6988 acc_val: 0.7285\n",
      "Epoch: [192/500] loss_train: 0.6272 acc_train: 0.7193 acc_val: 0.7324\n",
      "Epoch: [193/500] loss_train: 0.6377 acc_train: 0.7008 acc_val: 0.7344\n",
      "Epoch: [194/500] loss_train: 0.6269 acc_train: 0.6926 acc_val: 0.7363\n",
      "Epoch: [195/500] loss_train: 0.6250 acc_train: 0.7172 acc_val: 0.7402\n",
      "Epoch: [196/500] loss_train: 0.6313 acc_train: 0.7070 acc_val: 0.7441\n",
      "Epoch: [197/500] loss_train: 0.6288 acc_train: 0.7049 acc_val: 0.7461\n",
      "Epoch: [198/500] loss_train: 0.6219 acc_train: 0.7193 acc_val: 0.7480\n",
      "Epoch: [199/500] loss_train: 0.6291 acc_train: 0.7152 acc_val: 0.7500\n",
      "Epoch: [200/500] loss_train: 0.6226 acc_train: 0.7213 acc_val: 0.7539\n",
      "Epoch: [201/500] loss_train: 0.6195 acc_train: 0.7254 acc_val: 0.7539\n",
      "Epoch: [202/500] loss_train: 0.6184 acc_train: 0.7336 acc_val: 0.7598\n",
      "Epoch: [203/500] loss_train: 0.6251 acc_train: 0.7295 acc_val: 0.7617\n",
      "Epoch: [204/500] loss_train: 0.6183 acc_train: 0.7336 acc_val: 0.7676\n",
      "Epoch: [205/500] loss_train: 0.6145 acc_train: 0.7254 acc_val: 0.7754\n",
      "Epoch: [206/500] loss_train: 0.6176 acc_train: 0.7357 acc_val: 0.7832\n",
      "Epoch: [207/500] loss_train: 0.6133 acc_train: 0.7602 acc_val: 0.7812\n",
      "Epoch: [208/500] loss_train: 0.6164 acc_train: 0.7459 acc_val: 0.7812\n",
      "Epoch: [209/500] loss_train: 0.6112 acc_train: 0.7643 acc_val: 0.7910\n",
      "Epoch: [210/500] loss_train: 0.6090 acc_train: 0.7377 acc_val: 0.7969\n",
      "Epoch: [211/500] loss_train: 0.6078 acc_train: 0.7500 acc_val: 0.7949\n",
      "Epoch: [212/500] loss_train: 0.6052 acc_train: 0.7500 acc_val: 0.7949\n",
      "Epoch: [213/500] loss_train: 0.6124 acc_train: 0.7520 acc_val: 0.7930\n",
      "Epoch: [214/500] loss_train: 0.6068 acc_train: 0.7746 acc_val: 0.7871\n",
      "Epoch: [215/500] loss_train: 0.6059 acc_train: 0.7459 acc_val: 0.7871\n",
      "Epoch: [216/500] loss_train: 0.6010 acc_train: 0.7725 acc_val: 0.7910\n",
      "Epoch: [217/500] loss_train: 0.6028 acc_train: 0.7439 acc_val: 0.7891\n",
      "Epoch: [218/500] loss_train: 0.6059 acc_train: 0.7480 acc_val: 0.7871\n",
      "Epoch: [219/500] loss_train: 0.6065 acc_train: 0.7520 acc_val: 0.7832\n",
      "Epoch: [220/500] loss_train: 0.6022 acc_train: 0.7377 acc_val: 0.7871\n",
      "Epoch: [221/500] loss_train: 0.6014 acc_train: 0.7520 acc_val: 0.7871\n",
      "Epoch: [222/500] loss_train: 0.6088 acc_train: 0.7520 acc_val: 0.7852\n",
      "Epoch: [223/500] loss_train: 0.6029 acc_train: 0.7541 acc_val: 0.7871\n",
      "Epoch: [224/500] loss_train: 0.6049 acc_train: 0.7357 acc_val: 0.7910\n",
      "Epoch: [225/500] loss_train: 0.6009 acc_train: 0.7602 acc_val: 0.7930\n",
      "Epoch: [226/500] loss_train: 0.6007 acc_train: 0.7439 acc_val: 0.7910\n",
      "Epoch: [227/500] loss_train: 0.5947 acc_train: 0.7582 acc_val: 0.7949\n",
      "Epoch: [228/500] loss_train: 0.6019 acc_train: 0.7561 acc_val: 0.7988\n",
      "Epoch: [229/500] loss_train: 0.5932 acc_train: 0.7684 acc_val: 0.7969\n",
      "Epoch: [230/500] loss_train: 0.6013 acc_train: 0.7520 acc_val: 0.7969\n",
      "Epoch: [231/500] loss_train: 0.5876 acc_train: 0.7725 acc_val: 0.7988\n",
      "Epoch: [232/500] loss_train: 0.5948 acc_train: 0.7582 acc_val: 0.8027\n",
      "Epoch: [233/500] loss_train: 0.5896 acc_train: 0.7705 acc_val: 0.8008\n",
      "Epoch: [234/500] loss_train: 0.5886 acc_train: 0.7541 acc_val: 0.8047\n",
      "Epoch: [235/500] loss_train: 0.5960 acc_train: 0.7316 acc_val: 0.7988\n",
      "Epoch: [236/500] loss_train: 0.5934 acc_train: 0.7316 acc_val: 0.7910\n",
      "Epoch: [237/500] loss_train: 0.5912 acc_train: 0.7398 acc_val: 0.7949\n",
      "Epoch: [238/500] loss_train: 0.5927 acc_train: 0.7398 acc_val: 0.7910\n",
      "Epoch: [239/500] loss_train: 0.5897 acc_train: 0.7561 acc_val: 0.7910\n",
      "Epoch: [240/500] loss_train: 0.5854 acc_train: 0.7664 acc_val: 0.7910\n",
      "Epoch: [241/500] loss_train: 0.5877 acc_train: 0.7602 acc_val: 0.7871\n",
      "Epoch: [242/500] loss_train: 0.5875 acc_train: 0.7459 acc_val: 0.7793\n",
      "Epoch: [243/500] loss_train: 0.5841 acc_train: 0.7520 acc_val: 0.7773\n",
      "Epoch: [244/500] loss_train: 0.5885 acc_train: 0.7398 acc_val: 0.7734\n",
      "Epoch: [245/500] loss_train: 0.5886 acc_train: 0.7316 acc_val: 0.7715\n",
      "Epoch: [246/500] loss_train: 0.5811 acc_train: 0.7439 acc_val: 0.7715\n",
      "Epoch: [247/500] loss_train: 0.5871 acc_train: 0.7500 acc_val: 0.7695\n",
      "Epoch: [248/500] loss_train: 0.5841 acc_train: 0.7336 acc_val: 0.7695\n",
      "Epoch: [249/500] loss_train: 0.5884 acc_train: 0.7275 acc_val: 0.7695\n",
      "Epoch: [250/500] loss_train: 0.5819 acc_train: 0.7582 acc_val: 0.7676\n",
      "Epoch: [251/500] loss_train: 0.5899 acc_train: 0.7295 acc_val: 0.7676\n",
      "Epoch: [252/500] loss_train: 0.5869 acc_train: 0.7172 acc_val: 0.7656\n",
      "Epoch: [253/500] loss_train: 0.5797 acc_train: 0.7377 acc_val: 0.7637\n",
      "Epoch: [254/500] loss_train: 0.5810 acc_train: 0.7357 acc_val: 0.7637\n",
      "Epoch: [255/500] loss_train: 0.5896 acc_train: 0.7111 acc_val: 0.7656\n",
      "Epoch: [256/500] loss_train: 0.5872 acc_train: 0.7275 acc_val: 0.7637\n",
      "Epoch: [257/500] loss_train: 0.5724 acc_train: 0.7459 acc_val: 0.7617\n",
      "Epoch: [258/500] loss_train: 0.5858 acc_train: 0.7295 acc_val: 0.7578\n",
      "Epoch: [259/500] loss_train: 0.5798 acc_train: 0.7520 acc_val: 0.7539\n",
      "Epoch: [260/500] loss_train: 0.5735 acc_train: 0.7439 acc_val: 0.7539\n",
      "Epoch: [261/500] loss_train: 0.5815 acc_train: 0.7275 acc_val: 0.7520\n",
      "Epoch: [262/500] loss_train: 0.5808 acc_train: 0.7357 acc_val: 0.7520\n",
      "Epoch: [263/500] loss_train: 0.5715 acc_train: 0.7398 acc_val: 0.7520\n",
      "Epoch: [264/500] loss_train: 0.5793 acc_train: 0.7172 acc_val: 0.7520\n",
      "Epoch: [265/500] loss_train: 0.5788 acc_train: 0.7398 acc_val: 0.7500\n",
      "Epoch: [266/500] loss_train: 0.5785 acc_train: 0.7336 acc_val: 0.7500\n",
      "Epoch: [267/500] loss_train: 0.5772 acc_train: 0.7336 acc_val: 0.7500\n",
      "Epoch: [268/500] loss_train: 0.5856 acc_train: 0.7213 acc_val: 0.7441\n",
      "Epoch: [269/500] loss_train: 0.5780 acc_train: 0.7377 acc_val: 0.7441\n",
      "Epoch: [270/500] loss_train: 0.5791 acc_train: 0.7295 acc_val: 0.7422\n",
      "Epoch: [271/500] loss_train: 0.5777 acc_train: 0.7234 acc_val: 0.7422\n",
      "Epoch: [272/500] loss_train: 0.5806 acc_train: 0.7111 acc_val: 0.7441\n",
      "Epoch: [273/500] loss_train: 0.5712 acc_train: 0.7295 acc_val: 0.7441\n",
      "Epoch: [274/500] loss_train: 0.5787 acc_train: 0.7398 acc_val: 0.7422\n",
      "Epoch: [275/500] loss_train: 0.5774 acc_train: 0.7275 acc_val: 0.7422\n",
      "Epoch: [276/500] loss_train: 0.5784 acc_train: 0.7213 acc_val: 0.7422\n",
      "Epoch: [277/500] loss_train: 0.5759 acc_train: 0.7357 acc_val: 0.7441\n",
      "Epoch: [278/500] loss_train: 0.5749 acc_train: 0.7172 acc_val: 0.7461\n",
      "Epoch: [279/500] loss_train: 0.5787 acc_train: 0.7090 acc_val: 0.7500\n",
      "Epoch: [280/500] loss_train: 0.5846 acc_train: 0.6988 acc_val: 0.7500\n",
      "Epoch: [281/500] loss_train: 0.5730 acc_train: 0.7275 acc_val: 0.7500\n",
      "Epoch: [282/500] loss_train: 0.5714 acc_train: 0.7213 acc_val: 0.7500\n",
      "Epoch: [283/500] loss_train: 0.5707 acc_train: 0.7234 acc_val: 0.7500\n",
      "Epoch: [284/500] loss_train: 0.5762 acc_train: 0.7275 acc_val: 0.7500\n",
      "Epoch: [285/500] loss_train: 0.5753 acc_train: 0.7234 acc_val: 0.7500\n",
      "Epoch: [286/500] loss_train: 0.5730 acc_train: 0.7131 acc_val: 0.7520\n",
      "Epoch: [287/500] loss_train: 0.5716 acc_train: 0.7275 acc_val: 0.7520\n",
      "Epoch: [288/500] loss_train: 0.5689 acc_train: 0.7316 acc_val: 0.7520\n",
      "Epoch: [289/500] loss_train: 0.5789 acc_train: 0.7008 acc_val: 0.7520\n",
      "Epoch: [290/500] loss_train: 0.5705 acc_train: 0.7152 acc_val: 0.7520\n",
      "Epoch: [291/500] loss_train: 0.5689 acc_train: 0.7398 acc_val: 0.7539\n",
      "Epoch: [292/500] loss_train: 0.5830 acc_train: 0.6844 acc_val: 0.7520\n",
      "Epoch: [293/500] loss_train: 0.5809 acc_train: 0.7172 acc_val: 0.7539\n",
      "Epoch: [294/500] loss_train: 0.5739 acc_train: 0.7234 acc_val: 0.7520\n",
      "Epoch: [295/500] loss_train: 0.5705 acc_train: 0.7172 acc_val: 0.7520\n",
      "Epoch: [296/500] loss_train: 0.5696 acc_train: 0.7439 acc_val: 0.7520\n",
      "Epoch: [297/500] loss_train: 0.5699 acc_train: 0.7234 acc_val: 0.7520\n",
      "Epoch: [298/500] loss_train: 0.5711 acc_train: 0.7049 acc_val: 0.7520\n",
      "Epoch: [299/500] loss_train: 0.5780 acc_train: 0.6967 acc_val: 0.7520\n",
      "Epoch: [300/500] loss_train: 0.5750 acc_train: 0.6926 acc_val: 0.7500\n",
      "Epoch: [301/500] loss_train: 0.5699 acc_train: 0.7152 acc_val: 0.7500\n",
      "Epoch: [302/500] loss_train: 0.5754 acc_train: 0.7152 acc_val: 0.7500\n",
      "Epoch: [303/500] loss_train: 0.5629 acc_train: 0.7234 acc_val: 0.7500\n",
      "Epoch: [304/500] loss_train: 0.5690 acc_train: 0.7213 acc_val: 0.7500\n",
      "Epoch: [305/500] loss_train: 0.5722 acc_train: 0.7152 acc_val: 0.7500\n",
      "Epoch: [306/500] loss_train: 0.5676 acc_train: 0.7131 acc_val: 0.7500\n",
      "Epoch: [307/500] loss_train: 0.5687 acc_train: 0.7275 acc_val: 0.7500\n",
      "Epoch: [308/500] loss_train: 0.5607 acc_train: 0.7336 acc_val: 0.7500\n",
      "Epoch: [309/500] loss_train: 0.5731 acc_train: 0.7131 acc_val: 0.7500\n",
      "Epoch: [310/500] loss_train: 0.5679 acc_train: 0.7234 acc_val: 0.7500\n",
      "Epoch: [311/500] loss_train: 0.5685 acc_train: 0.7193 acc_val: 0.7500\n",
      "Epoch: [312/500] loss_train: 0.5656 acc_train: 0.7275 acc_val: 0.7500\n",
      "Epoch: [313/500] loss_train: 0.5624 acc_train: 0.7275 acc_val: 0.7500\n",
      "Epoch: [314/500] loss_train: 0.5668 acc_train: 0.7049 acc_val: 0.7500\n",
      "Epoch: [315/500] loss_train: 0.5648 acc_train: 0.7254 acc_val: 0.7500\n",
      "Epoch: [316/500] loss_train: 0.5693 acc_train: 0.7193 acc_val: 0.7500\n",
      "Epoch: [317/500] loss_train: 0.5717 acc_train: 0.6988 acc_val: 0.7520\n",
      "Epoch: [318/500] loss_train: 0.5563 acc_train: 0.7254 acc_val: 0.7520\n",
      "Epoch: [319/500] loss_train: 0.5502 acc_train: 0.7275 acc_val: 0.7520\n",
      "Epoch: [320/500] loss_train: 0.5610 acc_train: 0.7254 acc_val: 0.7520\n",
      "Epoch: [321/500] loss_train: 0.5547 acc_train: 0.7316 acc_val: 0.7520\n",
      "Epoch: [322/500] loss_train: 0.5733 acc_train: 0.6967 acc_val: 0.7520\n",
      "Epoch: [323/500] loss_train: 0.5650 acc_train: 0.7111 acc_val: 0.7520\n",
      "Epoch: [324/500] loss_train: 0.5656 acc_train: 0.7316 acc_val: 0.7520\n",
      "Epoch: [325/500] loss_train: 0.5675 acc_train: 0.7090 acc_val: 0.7520\n",
      "Epoch: [326/500] loss_train: 0.5640 acc_train: 0.7254 acc_val: 0.7520\n",
      "Epoch: [327/500] loss_train: 0.5591 acc_train: 0.7336 acc_val: 0.7520\n",
      "Epoch: [328/500] loss_train: 0.5640 acc_train: 0.7234 acc_val: 0.7520\n",
      "Epoch: [329/500] loss_train: 0.5537 acc_train: 0.7357 acc_val: 0.7520\n",
      "Epoch: [330/500] loss_train: 0.5694 acc_train: 0.7090 acc_val: 0.7520\n",
      "Epoch: [331/500] loss_train: 0.5518 acc_train: 0.7377 acc_val: 0.7520\n",
      "Epoch: [332/500] loss_train: 0.5604 acc_train: 0.7172 acc_val: 0.7539\n",
      "Epoch: [333/500] loss_train: 0.5543 acc_train: 0.7254 acc_val: 0.7539\n",
      "Epoch: [334/500] loss_train: 0.5498 acc_train: 0.7398 acc_val: 0.7539\n",
      "Epoch: [335/500] loss_train: 0.5726 acc_train: 0.7049 acc_val: 0.7559\n",
      "Epoch: [336/500] loss_train: 0.5559 acc_train: 0.7234 acc_val: 0.7559\n",
      "Epoch: [337/500] loss_train: 0.5479 acc_train: 0.7398 acc_val: 0.7559\n",
      "Epoch: [338/500] loss_train: 0.5575 acc_train: 0.7234 acc_val: 0.7559\n",
      "Epoch: [339/500] loss_train: 0.5568 acc_train: 0.7295 acc_val: 0.7559\n",
      "Epoch: [340/500] loss_train: 0.5519 acc_train: 0.7418 acc_val: 0.7559\n",
      "Epoch: [341/500] loss_train: 0.5570 acc_train: 0.7254 acc_val: 0.7559\n",
      "Epoch: [342/500] loss_train: 0.5560 acc_train: 0.7172 acc_val: 0.7559\n",
      "Epoch: [343/500] loss_train: 0.5475 acc_train: 0.7275 acc_val: 0.7578\n",
      "Epoch: [344/500] loss_train: 0.5587 acc_train: 0.7213 acc_val: 0.7578\n",
      "Epoch: [345/500] loss_train: 0.5444 acc_train: 0.7561 acc_val: 0.7578\n",
      "Epoch: [346/500] loss_train: 0.5463 acc_train: 0.7336 acc_val: 0.7598\n",
      "Epoch: [347/500] loss_train: 0.5586 acc_train: 0.7172 acc_val: 0.7598\n",
      "Epoch: [348/500] loss_train: 0.5527 acc_train: 0.7234 acc_val: 0.7598\n",
      "Epoch: [349/500] loss_train: 0.5374 acc_train: 0.7520 acc_val: 0.7617\n",
      "Epoch: [350/500] loss_train: 0.5522 acc_train: 0.7316 acc_val: 0.7617\n",
      "Epoch: [351/500] loss_train: 0.5481 acc_train: 0.7459 acc_val: 0.7598\n",
      "Epoch: [352/500] loss_train: 0.5594 acc_train: 0.7275 acc_val: 0.7617\n",
      "Epoch: [353/500] loss_train: 0.5421 acc_train: 0.7500 acc_val: 0.7617\n",
      "Epoch: [354/500] loss_train: 0.5453 acc_train: 0.7336 acc_val: 0.7617\n",
      "Epoch: [355/500] loss_train: 0.5497 acc_train: 0.7357 acc_val: 0.7617\n",
      "Epoch: [356/500] loss_train: 0.5474 acc_train: 0.7275 acc_val: 0.7617\n",
      "Epoch: [357/500] loss_train: 0.5434 acc_train: 0.7254 acc_val: 0.7637\n",
      "Epoch: [358/500] loss_train: 0.5422 acc_train: 0.7480 acc_val: 0.7637\n",
      "Epoch: [359/500] loss_train: 0.5464 acc_train: 0.7377 acc_val: 0.7637\n",
      "Epoch: [360/500] loss_train: 0.5473 acc_train: 0.7357 acc_val: 0.7637\n",
      "Epoch: [361/500] loss_train: 0.5413 acc_train: 0.7357 acc_val: 0.7637\n",
      "Epoch: [362/500] loss_train: 0.5527 acc_train: 0.7316 acc_val: 0.7637\n",
      "Epoch: [363/500] loss_train: 0.5433 acc_train: 0.7398 acc_val: 0.7637\n",
      "Epoch: [364/500] loss_train: 0.5480 acc_train: 0.7398 acc_val: 0.7656\n",
      "Epoch: [365/500] loss_train: 0.5448 acc_train: 0.7316 acc_val: 0.7656\n",
      "Epoch: [366/500] loss_train: 0.5374 acc_train: 0.7357 acc_val: 0.7656\n",
      "Epoch: [367/500] loss_train: 0.5316 acc_train: 0.7418 acc_val: 0.7656\n",
      "Epoch: [368/500] loss_train: 0.5324 acc_train: 0.7357 acc_val: 0.7656\n",
      "Epoch: [369/500] loss_train: 0.5363 acc_train: 0.7398 acc_val: 0.7656\n",
      "Epoch: [370/500] loss_train: 0.5408 acc_train: 0.7316 acc_val: 0.7676\n",
      "Epoch: [371/500] loss_train: 0.5551 acc_train: 0.7213 acc_val: 0.7695\n",
      "Epoch: [372/500] loss_train: 0.5410 acc_train: 0.7500 acc_val: 0.7695\n",
      "Epoch: [373/500] loss_train: 0.5384 acc_train: 0.7357 acc_val: 0.7715\n",
      "Epoch: [374/500] loss_train: 0.5249 acc_train: 0.7541 acc_val: 0.7715\n",
      "Epoch: [375/500] loss_train: 0.5346 acc_train: 0.7418 acc_val: 0.7715\n",
      "Epoch: [376/500] loss_train: 0.5373 acc_train: 0.7357 acc_val: 0.7734\n",
      "Epoch: [377/500] loss_train: 0.5432 acc_train: 0.7357 acc_val: 0.7734\n",
      "Epoch: [378/500] loss_train: 0.5419 acc_train: 0.7275 acc_val: 0.7715\n",
      "Epoch: [379/500] loss_train: 0.5269 acc_train: 0.7684 acc_val: 0.7734\n",
      "Epoch: [380/500] loss_train: 0.5266 acc_train: 0.7520 acc_val: 0.7734\n",
      "Epoch: [381/500] loss_train: 0.5302 acc_train: 0.7377 acc_val: 0.7715\n",
      "Epoch: [382/500] loss_train: 0.5255 acc_train: 0.7520 acc_val: 0.7734\n",
      "Epoch: [383/500] loss_train: 0.5420 acc_train: 0.7480 acc_val: 0.7734\n",
      "Epoch: [384/500] loss_train: 0.5329 acc_train: 0.7398 acc_val: 0.7754\n",
      "Epoch: [385/500] loss_train: 0.5332 acc_train: 0.7459 acc_val: 0.7734\n",
      "Epoch: [386/500] loss_train: 0.5280 acc_train: 0.7561 acc_val: 0.7734\n",
      "Epoch: [387/500] loss_train: 0.5413 acc_train: 0.7398 acc_val: 0.7734\n",
      "Epoch: [388/500] loss_train: 0.5380 acc_train: 0.7377 acc_val: 0.7734\n",
      "Epoch: [389/500] loss_train: 0.5269 acc_train: 0.7623 acc_val: 0.7715\n",
      "Epoch: [390/500] loss_train: 0.5155 acc_train: 0.7623 acc_val: 0.7715\n",
      "Epoch: [391/500] loss_train: 0.5245 acc_train: 0.7602 acc_val: 0.7715\n",
      "Epoch: [392/500] loss_train: 0.5292 acc_train: 0.7541 acc_val: 0.7734\n",
      "Epoch: [393/500] loss_train: 0.5330 acc_train: 0.7398 acc_val: 0.7734\n",
      "Epoch: [394/500] loss_train: 0.5166 acc_train: 0.7541 acc_val: 0.7773\n",
      "Epoch: [395/500] loss_train: 0.5312 acc_train: 0.7480 acc_val: 0.7773\n",
      "Epoch: [396/500] loss_train: 0.5208 acc_train: 0.7561 acc_val: 0.7773\n",
      "Epoch: [397/500] loss_train: 0.5278 acc_train: 0.7520 acc_val: 0.7793\n",
      "Epoch: [398/500] loss_train: 0.5266 acc_train: 0.7459 acc_val: 0.7793\n",
      "Epoch: [399/500] loss_train: 0.5292 acc_train: 0.7336 acc_val: 0.7793\n",
      "Epoch: [400/500] loss_train: 0.5252 acc_train: 0.7561 acc_val: 0.7793\n",
      "Epoch: [401/500] loss_train: 0.5281 acc_train: 0.7459 acc_val: 0.7793\n",
      "Epoch: [402/500] loss_train: 0.5155 acc_train: 0.7602 acc_val: 0.7793\n",
      "Epoch: [403/500] loss_train: 0.5144 acc_train: 0.7602 acc_val: 0.7793\n",
      "Epoch: [404/500] loss_train: 0.5068 acc_train: 0.7828 acc_val: 0.7793\n",
      "Epoch: [405/500] loss_train: 0.5188 acc_train: 0.7602 acc_val: 0.7793\n",
      "Epoch: [406/500] loss_train: 0.5254 acc_train: 0.7541 acc_val: 0.7793\n",
      "Epoch: [407/500] loss_train: 0.5231 acc_train: 0.7623 acc_val: 0.7812\n",
      "Epoch: [408/500] loss_train: 0.5241 acc_train: 0.7541 acc_val: 0.7812\n",
      "Epoch: [409/500] loss_train: 0.5095 acc_train: 0.7746 acc_val: 0.7812\n",
      "Epoch: [410/500] loss_train: 0.5106 acc_train: 0.7746 acc_val: 0.7832\n",
      "Epoch: [411/500] loss_train: 0.5180 acc_train: 0.7705 acc_val: 0.7832\n",
      "Epoch: [412/500] loss_train: 0.5088 acc_train: 0.7787 acc_val: 0.7812\n",
      "Epoch: [413/500] loss_train: 0.5135 acc_train: 0.7520 acc_val: 0.7812\n",
      "Epoch: [414/500] loss_train: 0.5258 acc_train: 0.7500 acc_val: 0.7812\n",
      "Epoch: [415/500] loss_train: 0.5060 acc_train: 0.7807 acc_val: 0.7812\n",
      "Epoch: [416/500] loss_train: 0.5142 acc_train: 0.7684 acc_val: 0.7812\n",
      "Epoch: [417/500] loss_train: 0.5300 acc_train: 0.7541 acc_val: 0.7852\n",
      "Epoch: [418/500] loss_train: 0.5103 acc_train: 0.7807 acc_val: 0.7852\n",
      "Epoch: [419/500] loss_train: 0.5170 acc_train: 0.7623 acc_val: 0.7871\n",
      "Epoch: [420/500] loss_train: 0.5177 acc_train: 0.7664 acc_val: 0.7871\n",
      "Epoch: [421/500] loss_train: 0.5233 acc_train: 0.7582 acc_val: 0.7871\n",
      "Epoch: [422/500] loss_train: 0.5191 acc_train: 0.7541 acc_val: 0.7891\n",
      "Epoch: [423/500] loss_train: 0.5056 acc_train: 0.7828 acc_val: 0.7891\n",
      "Epoch: [424/500] loss_train: 0.4987 acc_train: 0.7848 acc_val: 0.7891\n",
      "Epoch: [425/500] loss_train: 0.5104 acc_train: 0.7684 acc_val: 0.7891\n",
      "Epoch: [426/500] loss_train: 0.5146 acc_train: 0.7684 acc_val: 0.7891\n",
      "Epoch: [427/500] loss_train: 0.5225 acc_train: 0.7480 acc_val: 0.7891\n",
      "Epoch: [428/500] loss_train: 0.5132 acc_train: 0.7602 acc_val: 0.7891\n",
      "Epoch: [429/500] loss_train: 0.5042 acc_train: 0.7828 acc_val: 0.7891\n",
      "Epoch: [430/500] loss_train: 0.5104 acc_train: 0.7664 acc_val: 0.7891\n",
      "Epoch: [431/500] loss_train: 0.5078 acc_train: 0.7766 acc_val: 0.7891\n",
      "Epoch: [432/500] loss_train: 0.4980 acc_train: 0.7828 acc_val: 0.7910\n",
      "Epoch: [433/500] loss_train: 0.5005 acc_train: 0.7807 acc_val: 0.7910\n",
      "Epoch: [434/500] loss_train: 0.5082 acc_train: 0.7643 acc_val: 0.7930\n",
      "Epoch: [435/500] loss_train: 0.5100 acc_train: 0.7787 acc_val: 0.7930\n",
      "Epoch: [436/500] loss_train: 0.5223 acc_train: 0.7500 acc_val: 0.7930\n",
      "Epoch: [437/500] loss_train: 0.5105 acc_train: 0.7848 acc_val: 0.7949\n",
      "Epoch: [438/500] loss_train: 0.5052 acc_train: 0.7705 acc_val: 0.7988\n",
      "Epoch: [439/500] loss_train: 0.5025 acc_train: 0.7828 acc_val: 0.8008\n",
      "Epoch: [440/500] loss_train: 0.4962 acc_train: 0.7951 acc_val: 0.8047\n",
      "Epoch: [441/500] loss_train: 0.5116 acc_train: 0.7725 acc_val: 0.8066\n",
      "Epoch: [442/500] loss_train: 0.5011 acc_train: 0.7910 acc_val: 0.8047\n",
      "Epoch: [443/500] loss_train: 0.5156 acc_train: 0.7664 acc_val: 0.8047\n",
      "Epoch: [444/500] loss_train: 0.5095 acc_train: 0.7766 acc_val: 0.8047\n",
      "Epoch: [445/500] loss_train: 0.5128 acc_train: 0.7664 acc_val: 0.8066\n",
      "Epoch: [446/500] loss_train: 0.5018 acc_train: 0.7889 acc_val: 0.8086\n",
      "Acc_best is updated to 0.8086. Model checkpoint is saved to ./model/BA1k_MVC_Teacher.pt.\n",
      "Test accuracy is 0.7960\n",
      "Epoch: [447/500] loss_train: 0.4965 acc_train: 0.7910 acc_val: 0.8066\n",
      "Epoch: [448/500] loss_train: 0.5016 acc_train: 0.7766 acc_val: 0.8086\n",
      "Epoch: [449/500] loss_train: 0.5075 acc_train: 0.7725 acc_val: 0.8086\n",
      "Epoch: [450/500] loss_train: 0.5081 acc_train: 0.7746 acc_val: 0.8086\n",
      "Epoch: [451/500] loss_train: 0.5077 acc_train: 0.7725 acc_val: 0.8086\n",
      "Epoch: [452/500] loss_train: 0.5024 acc_train: 0.7910 acc_val: 0.8086\n",
      "Epoch: [453/500] loss_train: 0.4899 acc_train: 0.7971 acc_val: 0.8086\n",
      "Epoch: [454/500] loss_train: 0.5050 acc_train: 0.7746 acc_val: 0.8086\n",
      "Epoch: [455/500] loss_train: 0.4932 acc_train: 0.7848 acc_val: 0.8086\n",
      "Epoch: [456/500] loss_train: 0.5046 acc_train: 0.7828 acc_val: 0.8086\n",
      "Epoch: [457/500] loss_train: 0.4884 acc_train: 0.7951 acc_val: 0.8086\n",
      "Epoch: [458/500] loss_train: 0.5116 acc_train: 0.7787 acc_val: 0.8086\n",
      "Epoch: [459/500] loss_train: 0.4969 acc_train: 0.7684 acc_val: 0.8105\n",
      "Acc_best is updated to 0.8105. Model checkpoint is saved to ./model/BA1k_MVC_Teacher.pt.\n",
      "Test accuracy is 0.8030\n",
      "Epoch: [460/500] loss_train: 0.4926 acc_train: 0.7930 acc_val: 0.8125\n",
      "Acc_best is updated to 0.8125. Model checkpoint is saved to ./model/BA1k_MVC_Teacher.pt.\n",
      "Test accuracy is 0.8040\n",
      "Epoch: [461/500] loss_train: 0.5024 acc_train: 0.7889 acc_val: 0.8125\n",
      "Epoch: [462/500] loss_train: 0.4968 acc_train: 0.7889 acc_val: 0.8145\n",
      "Acc_best is updated to 0.8145. Model checkpoint is saved to ./model/BA1k_MVC_Teacher.pt.\n",
      "Test accuracy is 0.8050\n",
      "Epoch: [463/500] loss_train: 0.5056 acc_train: 0.7807 acc_val: 0.8125\n",
      "Epoch: [464/500] loss_train: 0.5105 acc_train: 0.7684 acc_val: 0.8125\n",
      "Epoch: [465/500] loss_train: 0.4937 acc_train: 0.7869 acc_val: 0.8125\n",
      "Epoch: [466/500] loss_train: 0.4887 acc_train: 0.7971 acc_val: 0.8125\n",
      "Epoch: [467/500] loss_train: 0.4920 acc_train: 0.7930 acc_val: 0.8125\n",
      "Epoch: [468/500] loss_train: 0.4942 acc_train: 0.7848 acc_val: 0.8125\n",
      "Epoch: [469/500] loss_train: 0.5026 acc_train: 0.7848 acc_val: 0.8125\n",
      "Epoch: [470/500] loss_train: 0.4968 acc_train: 0.7787 acc_val: 0.8125\n",
      "Epoch: [471/500] loss_train: 0.4922 acc_train: 0.7992 acc_val: 0.8125\n",
      "Epoch: [472/500] loss_train: 0.5048 acc_train: 0.7766 acc_val: 0.8125\n",
      "Epoch: [473/500] loss_train: 0.5018 acc_train: 0.7787 acc_val: 0.8125\n",
      "Epoch: [474/500] loss_train: 0.5120 acc_train: 0.7725 acc_val: 0.8125\n",
      "Epoch: [475/500] loss_train: 0.4973 acc_train: 0.7787 acc_val: 0.8125\n",
      "Epoch: [476/500] loss_train: 0.4972 acc_train: 0.7725 acc_val: 0.8145\n",
      "Epoch: [477/500] loss_train: 0.5004 acc_train: 0.7828 acc_val: 0.8145\n",
      "Epoch: [478/500] loss_train: 0.4992 acc_train: 0.7889 acc_val: 0.8105\n",
      "Epoch: [479/500] loss_train: 0.5033 acc_train: 0.7725 acc_val: 0.8105\n",
      "Epoch: [480/500] loss_train: 0.5024 acc_train: 0.7848 acc_val: 0.8105\n",
      "Epoch: [481/500] loss_train: 0.4903 acc_train: 0.7971 acc_val: 0.8105\n",
      "Epoch: [482/500] loss_train: 0.5006 acc_train: 0.7787 acc_val: 0.8066\n",
      "Epoch: [483/500] loss_train: 0.4879 acc_train: 0.7951 acc_val: 0.8066\n",
      "Epoch: [484/500] loss_train: 0.4954 acc_train: 0.7910 acc_val: 0.8086\n",
      "Epoch: [485/500] loss_train: 0.5190 acc_train: 0.7643 acc_val: 0.8105\n",
      "Epoch: [486/500] loss_train: 0.5013 acc_train: 0.7848 acc_val: 0.8105\n",
      "Epoch: [487/500] loss_train: 0.4905 acc_train: 0.8094 acc_val: 0.8086\n",
      "Epoch: [488/500] loss_train: 0.4793 acc_train: 0.8074 acc_val: 0.8086\n",
      "Epoch: [489/500] loss_train: 0.4874 acc_train: 0.7992 acc_val: 0.8086\n",
      "Epoch: [490/500] loss_train: 0.4986 acc_train: 0.7807 acc_val: 0.8086\n",
      "Epoch: [491/500] loss_train: 0.4911 acc_train: 0.7848 acc_val: 0.8086\n",
      "Epoch: [492/500] loss_train: 0.4955 acc_train: 0.7971 acc_val: 0.8086\n",
      "Epoch: [493/500] loss_train: 0.4935 acc_train: 0.7930 acc_val: 0.8086\n",
      "Epoch: [494/500] loss_train: 0.4904 acc_train: 0.7910 acc_val: 0.8086\n",
      "Epoch: [495/500] loss_train: 0.4951 acc_train: 0.7828 acc_val: 0.8086\n",
      "Epoch: [496/500] loss_train: 0.4842 acc_train: 0.7951 acc_val: 0.8086\n",
      "Epoch: [497/500] loss_train: 0.4812 acc_train: 0.7971 acc_val: 0.8086\n",
      "Epoch: [498/500] loss_train: 0.4797 acc_train: 0.7910 acc_val: 0.8105\n",
      "Epoch: [499/500] loss_train: 0.4865 acc_train: 0.7992 acc_val: 0.8105\n",
      "Epoch: [500/500] loss_train: 0.4862 acc_train: 0.7951 acc_val: 0.8086\n",
      "Final accuracy is 0.8050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min\n",
      "Wall time: 39.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%run teacher_mvc.py\n",
    "\n",
    "# CPU times: total: 3min 25s\n",
    "# Wall time: 1min 33s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading config...\n",
      "Loading dataset...\n",
      "Loading teacher model...\n",
      "Teacher GNN backbone is GraphSAGE\n",
      "In channels: 1024\n",
      "Hidden channels: 128\n",
      "Out channels: 2\n",
      "Loading weights...\n",
      "Get student model\n",
      "Student GNN backbone is GraphSAGE\n",
      "In channels: 1024\n",
      "Hidden channels: 32\n",
      "Out channels: 2\n",
      "Reseting model parameters...\n",
      "Get optimizer\n",
      "Learning rate: 0.0001\n",
      "Weight decay: 0.0005\n",
      "Start training...\n",
      "Epoch: [1/1000] loss_train: 0.3173 acc_train: 0.5840 acc_val: 0.6289\n",
      "Acc_best is updated to 0.6289. Model checkpoint is saved to ./model/BA1k_MVC_Student.pt\n",
      "Test accuracy is 0.6350\n",
      "Epoch: [2/1000] loss_train: 0.2867 acc_train: 0.5656 acc_val: 0.5254\n",
      "Epoch: [3/1000] loss_train: 0.3051 acc_train: 0.4836 acc_val: 0.4922\n",
      "Epoch: [4/1000] loss_train: 0.3146 acc_train: 0.4631 acc_val: 0.4922\n",
      "Epoch: [5/1000] loss_train: 0.3013 acc_train: 0.5143 acc_val: 0.5059\n",
      "Epoch: [6/1000] loss_train: 0.2880 acc_train: 0.5307 acc_val: 0.6289\n",
      "Epoch: [7/1000] loss_train: 0.2708 acc_train: 0.5902 acc_val: 0.7617\n",
      "Acc_best is updated to 0.7617. Model checkpoint is saved to ./model/BA1k_MVC_Student.pt\n",
      "Test accuracy is 0.7400\n",
      "Epoch: [8/1000] loss_train: 0.2807 acc_train: 0.5492 acc_val: 0.8047\n",
      "Acc_best is updated to 0.8047. Model checkpoint is saved to ./model/BA1k_MVC_Student.pt\n",
      "Test accuracy is 0.7800\n",
      "Epoch: [9/1000] loss_train: 0.2547 acc_train: 0.6332 acc_val: 0.7930\n",
      "Epoch: [10/1000] loss_train: 0.2564 acc_train: 0.6127 acc_val: 0.7871\n",
      "Epoch: [11/1000] loss_train: 0.2408 acc_train: 0.6660 acc_val: 0.7832\n",
      "Epoch: [12/1000] loss_train: 0.2443 acc_train: 0.6496 acc_val: 0.7617\n",
      "Epoch: [13/1000] loss_train: 0.2507 acc_train: 0.6455 acc_val: 0.7598\n",
      "Epoch: [14/1000] loss_train: 0.2389 acc_train: 0.6066 acc_val: 0.7598\n",
      "Epoch: [15/1000] loss_train: 0.2497 acc_train: 0.6414 acc_val: 0.7598\n",
      "Epoch: [16/1000] loss_train: 0.2642 acc_train: 0.6332 acc_val: 0.7734\n",
      "Epoch: [17/1000] loss_train: 0.2450 acc_train: 0.6598 acc_val: 0.7793\n",
      "Epoch: [18/1000] loss_train: 0.2411 acc_train: 0.6291 acc_val: 0.7832\n",
      "Epoch: [19/1000] loss_train: 0.2434 acc_train: 0.6680 acc_val: 0.8027\n",
      "Epoch: [20/1000] loss_train: 0.2438 acc_train: 0.6475 acc_val: 0.8105\n",
      "Acc_best is updated to 0.8105. Model checkpoint is saved to ./model/BA1k_MVC_Student.pt\n",
      "Test accuracy is 0.7910\n",
      "Epoch: [21/1000] loss_train: 0.2367 acc_train: 0.6311 acc_val: 0.8066\n",
      "Epoch: [22/1000] loss_train: 0.2308 acc_train: 0.6434 acc_val: 0.8027\n",
      "Epoch: [23/1000] loss_train: 0.2387 acc_train: 0.6373 acc_val: 0.7930\n",
      "Epoch: [24/1000] loss_train: 0.2455 acc_train: 0.6025 acc_val: 0.7520\n",
      "Epoch: [25/1000] loss_train: 0.2395 acc_train: 0.5984 acc_val: 0.7031\n",
      "Epoch: [26/1000] loss_train: 0.2372 acc_train: 0.6311 acc_val: 0.6641\n",
      "Epoch: [27/1000] loss_train: 0.2331 acc_train: 0.5840 acc_val: 0.6328\n",
      "Epoch: [28/1000] loss_train: 0.2225 acc_train: 0.6189 acc_val: 0.6074\n",
      "Epoch: [29/1000] loss_train: 0.2479 acc_train: 0.5656 acc_val: 0.5820\n",
      "Epoch: [30/1000] loss_train: 0.2323 acc_train: 0.5881 acc_val: 0.5566\n",
      "Epoch: [31/1000] loss_train: 0.2454 acc_train: 0.5328 acc_val: 0.5508\n",
      "Epoch: [32/1000] loss_train: 0.2225 acc_train: 0.6209 acc_val: 0.5410\n",
      "Epoch: [33/1000] loss_train: 0.2473 acc_train: 0.5553 acc_val: 0.5410\n",
      "Epoch: [34/1000] loss_train: 0.2339 acc_train: 0.5656 acc_val: 0.5547\n",
      "Epoch: [35/1000] loss_train: 0.2364 acc_train: 0.5574 acc_val: 0.5645\n",
      "Epoch: [36/1000] loss_train: 0.2374 acc_train: 0.5861 acc_val: 0.5820\n",
      "Epoch: [37/1000] loss_train: 0.2277 acc_train: 0.5881 acc_val: 0.5957\n",
      "Epoch: [38/1000] loss_train: 0.2307 acc_train: 0.5758 acc_val: 0.6270\n",
      "Epoch: [39/1000] loss_train: 0.2320 acc_train: 0.6004 acc_val: 0.6504\n",
      "Epoch: [40/1000] loss_train: 0.2223 acc_train: 0.6332 acc_val: 0.6816\n",
      "Epoch: [41/1000] loss_train: 0.2251 acc_train: 0.6393 acc_val: 0.7207\n",
      "Epoch: [42/1000] loss_train: 0.2277 acc_train: 0.6209 acc_val: 0.7754\n",
      "Epoch: [43/1000] loss_train: 0.2206 acc_train: 0.6455 acc_val: 0.7988\n",
      "Epoch: [44/1000] loss_train: 0.2168 acc_train: 0.6496 acc_val: 0.8145\n",
      "Acc_best is updated to 0.8145. Model checkpoint is saved to ./model/BA1k_MVC_Student.pt\n",
      "Test accuracy is 0.7790\n",
      "Epoch: [45/1000] loss_train: 0.2186 acc_train: 0.6557 acc_val: 0.8242\n",
      "Acc_best is updated to 0.8242. Model checkpoint is saved to ./model/BA1k_MVC_Student.pt\n",
      "Test accuracy is 0.8030\n",
      "Epoch: [46/1000] loss_train: 0.2115 acc_train: 0.6639 acc_val: 0.8262\n",
      "Acc_best is updated to 0.8262. Model checkpoint is saved to ./model/BA1k_MVC_Student.pt\n",
      "Test accuracy is 0.8010\n",
      "Epoch: [47/1000] loss_train: 0.2134 acc_train: 0.6742 acc_val: 0.8047\n",
      "Epoch: [48/1000] loss_train: 0.2151 acc_train: 0.6762 acc_val: 0.8047\n",
      "Epoch: [49/1000] loss_train: 0.2196 acc_train: 0.6680 acc_val: 0.7891\n",
      "Epoch: [50/1000] loss_train: 0.2169 acc_train: 0.6578 acc_val: 0.7656\n",
      "Epoch: [51/1000] loss_train: 0.2220 acc_train: 0.6434 acc_val: 0.7500\n",
      "Epoch: [52/1000] loss_train: 0.2179 acc_train: 0.6598 acc_val: 0.7227\n",
      "Epoch: [53/1000] loss_train: 0.2170 acc_train: 0.6516 acc_val: 0.6641\n",
      "Epoch: [54/1000] loss_train: 0.2172 acc_train: 0.6352 acc_val: 0.6055\n",
      "Epoch: [55/1000] loss_train: 0.2208 acc_train: 0.6537 acc_val: 0.5488\n",
      "Epoch: [56/1000] loss_train: 0.2205 acc_train: 0.6311 acc_val: 0.5293\n",
      "Epoch: [57/1000] loss_train: 0.2120 acc_train: 0.6578 acc_val: 0.5176\n",
      "Epoch: [58/1000] loss_train: 0.2201 acc_train: 0.6414 acc_val: 0.5137\n",
      "Epoch: [59/1000] loss_train: 0.2228 acc_train: 0.6393 acc_val: 0.5137\n",
      "Epoch: [60/1000] loss_train: 0.2265 acc_train: 0.6086 acc_val: 0.5137\n",
      "Epoch: [61/1000] loss_train: 0.2236 acc_train: 0.6291 acc_val: 0.5137\n",
      "Epoch: [62/1000] loss_train: 0.2312 acc_train: 0.6086 acc_val: 0.5137\n",
      "Epoch: [63/1000] loss_train: 0.2238 acc_train: 0.6455 acc_val: 0.5137\n",
      "Epoch: [64/1000] loss_train: 0.2271 acc_train: 0.6270 acc_val: 0.5137\n",
      "Epoch: [65/1000] loss_train: 0.2393 acc_train: 0.6189 acc_val: 0.5137\n",
      "Epoch: [66/1000] loss_train: 0.2409 acc_train: 0.6107 acc_val: 0.5137\n",
      "Epoch: [67/1000] loss_train: 0.2309 acc_train: 0.6168 acc_val: 0.5137\n",
      "Epoch: [68/1000] loss_train: 0.2393 acc_train: 0.5963 acc_val: 0.5137\n",
      "Epoch: [69/1000] loss_train: 0.2352 acc_train: 0.6004 acc_val: 0.5137\n",
      "Epoch: [70/1000] loss_train: 0.2239 acc_train: 0.6414 acc_val: 0.5137\n",
      "Epoch: [71/1000] loss_train: 0.2335 acc_train: 0.6045 acc_val: 0.5137\n",
      "Epoch: [72/1000] loss_train: 0.2249 acc_train: 0.6066 acc_val: 0.5137\n",
      "Epoch: [73/1000] loss_train: 0.2365 acc_train: 0.6045 acc_val: 0.5137\n",
      "Epoch: [74/1000] loss_train: 0.2271 acc_train: 0.6209 acc_val: 0.5137\n",
      "Epoch: [75/1000] loss_train: 0.2317 acc_train: 0.6127 acc_val: 0.5137\n",
      "Epoch: [76/1000] loss_train: 0.2274 acc_train: 0.6127 acc_val: 0.5137\n",
      "Epoch: [77/1000] loss_train: 0.2250 acc_train: 0.6148 acc_val: 0.5137\n",
      "Epoch: [78/1000] loss_train: 0.2249 acc_train: 0.6148 acc_val: 0.5137\n",
      "Epoch: [79/1000] loss_train: 0.2227 acc_train: 0.6250 acc_val: 0.5137\n",
      "Epoch: [80/1000] loss_train: 0.2211 acc_train: 0.6373 acc_val: 0.5156\n",
      "Epoch: [81/1000] loss_train: 0.2197 acc_train: 0.6537 acc_val: 0.5195\n",
      "Epoch: [82/1000] loss_train: 0.2176 acc_train: 0.6455 acc_val: 0.5312\n",
      "Epoch: [83/1000] loss_train: 0.2175 acc_train: 0.6250 acc_val: 0.5410\n",
      "Epoch: [84/1000] loss_train: 0.2156 acc_train: 0.6537 acc_val: 0.5684\n",
      "Epoch: [85/1000] loss_train: 0.2122 acc_train: 0.6475 acc_val: 0.6016\n",
      "Epoch: [86/1000] loss_train: 0.2208 acc_train: 0.6045 acc_val: 0.6328\n",
      "Epoch: [87/1000] loss_train: 0.2161 acc_train: 0.6496 acc_val: 0.6738\n",
      "Epoch: [88/1000] loss_train: 0.2077 acc_train: 0.6660 acc_val: 0.7051\n",
      "Epoch: [89/1000] loss_train: 0.2163 acc_train: 0.6414 acc_val: 0.7188\n",
      "Epoch: [90/1000] loss_train: 0.2088 acc_train: 0.6619 acc_val: 0.7344\n",
      "Epoch: [91/1000] loss_train: 0.2104 acc_train: 0.6742 acc_val: 0.7461\n",
      "Epoch: [92/1000] loss_train: 0.2154 acc_train: 0.6557 acc_val: 0.7578\n",
      "Epoch: [93/1000] loss_train: 0.2074 acc_train: 0.6926 acc_val: 0.7695\n",
      "Epoch: [94/1000] loss_train: 0.2084 acc_train: 0.6947 acc_val: 0.7754\n",
      "Epoch: [95/1000] loss_train: 0.2083 acc_train: 0.6988 acc_val: 0.7852\n",
      "Epoch: [96/1000] loss_train: 0.2084 acc_train: 0.6557 acc_val: 0.7793\n",
      "Epoch: [97/1000] loss_train: 0.2089 acc_train: 0.7049 acc_val: 0.7812\n",
      "Epoch: [98/1000] loss_train: 0.2063 acc_train: 0.6947 acc_val: 0.7910\n",
      "Epoch: [99/1000] loss_train: 0.2056 acc_train: 0.7049 acc_val: 0.7969\n",
      "Epoch: [100/1000] loss_train: 0.2054 acc_train: 0.7029 acc_val: 0.8008\n",
      "Epoch: [101/1000] loss_train: 0.2100 acc_train: 0.6680 acc_val: 0.8164\n",
      "Epoch: [102/1000] loss_train: 0.2125 acc_train: 0.6557 acc_val: 0.8125\n",
      "Epoch: [103/1000] loss_train: 0.2115 acc_train: 0.6803 acc_val: 0.8203\n",
      "Epoch: [104/1000] loss_train: 0.2097 acc_train: 0.6926 acc_val: 0.8203\n",
      "Epoch: [105/1000] loss_train: 0.2116 acc_train: 0.6783 acc_val: 0.8242\n",
      "Epoch: [106/1000] loss_train: 0.2074 acc_train: 0.7152 acc_val: 0.8262\n",
      "Epoch: [107/1000] loss_train: 0.2073 acc_train: 0.7131 acc_val: 0.8145\n",
      "Epoch: [108/1000] loss_train: 0.2091 acc_train: 0.7111 acc_val: 0.8164\n",
      "Epoch: [109/1000] loss_train: 0.2086 acc_train: 0.6844 acc_val: 0.8145\n",
      "Epoch: [110/1000] loss_train: 0.2058 acc_train: 0.6988 acc_val: 0.8184\n",
      "Epoch: [111/1000] loss_train: 0.2059 acc_train: 0.7008 acc_val: 0.8145\n",
      "Epoch: [112/1000] loss_train: 0.2127 acc_train: 0.6865 acc_val: 0.8008\n",
      "Epoch: [113/1000] loss_train: 0.2097 acc_train: 0.6639 acc_val: 0.7969\n",
      "Epoch: [114/1000] loss_train: 0.2157 acc_train: 0.6537 acc_val: 0.7949\n",
      "Epoch: [115/1000] loss_train: 0.2114 acc_train: 0.6742 acc_val: 0.7910\n",
      "Epoch: [116/1000] loss_train: 0.2134 acc_train: 0.6660 acc_val: 0.7891\n",
      "Epoch: [117/1000] loss_train: 0.2157 acc_train: 0.6393 acc_val: 0.7812\n",
      "Epoch: [118/1000] loss_train: 0.2148 acc_train: 0.6455 acc_val: 0.7715\n",
      "Epoch: [119/1000] loss_train: 0.2164 acc_train: 0.6783 acc_val: 0.7676\n",
      "Epoch: [120/1000] loss_train: 0.2108 acc_train: 0.6906 acc_val: 0.7656\n",
      "Epoch: [121/1000] loss_train: 0.2179 acc_train: 0.6557 acc_val: 0.7578\n",
      "Epoch: [122/1000] loss_train: 0.2159 acc_train: 0.6598 acc_val: 0.7480\n",
      "Epoch: [123/1000] loss_train: 0.2141 acc_train: 0.6496 acc_val: 0.7402\n",
      "Epoch: [124/1000] loss_train: 0.2145 acc_train: 0.6639 acc_val: 0.7363\n",
      "Epoch: [125/1000] loss_train: 0.2060 acc_train: 0.7152 acc_val: 0.7285\n",
      "Epoch: [126/1000] loss_train: 0.2156 acc_train: 0.6270 acc_val: 0.7246\n",
      "Epoch: [127/1000] loss_train: 0.2198 acc_train: 0.6127 acc_val: 0.7207\n",
      "Epoch: [128/1000] loss_train: 0.2206 acc_train: 0.6045 acc_val: 0.7188\n",
      "Epoch: [129/1000] loss_train: 0.2198 acc_train: 0.6352 acc_val: 0.7188\n",
      "Epoch: [130/1000] loss_train: 0.2155 acc_train: 0.6578 acc_val: 0.7148\n",
      "Epoch: [131/1000] loss_train: 0.2137 acc_train: 0.6414 acc_val: 0.7109\n",
      "Epoch: [132/1000] loss_train: 0.2174 acc_train: 0.6414 acc_val: 0.7070\n",
      "Epoch: [133/1000] loss_train: 0.2158 acc_train: 0.6332 acc_val: 0.7070\n",
      "Epoch: [134/1000] loss_train: 0.2101 acc_train: 0.6926 acc_val: 0.7070\n",
      "Epoch: [135/1000] loss_train: 0.2187 acc_train: 0.6127 acc_val: 0.7090\n",
      "Epoch: [136/1000] loss_train: 0.2141 acc_train: 0.6189 acc_val: 0.7109\n",
      "Epoch: [137/1000] loss_train: 0.2181 acc_train: 0.6373 acc_val: 0.7109\n",
      "Epoch: [138/1000] loss_train: 0.2131 acc_train: 0.6578 acc_val: 0.7129\n",
      "Epoch: [139/1000] loss_train: 0.2133 acc_train: 0.6414 acc_val: 0.7168\n",
      "Epoch: [140/1000] loss_train: 0.2180 acc_train: 0.6352 acc_val: 0.7188\n",
      "Epoch: [141/1000] loss_train: 0.2122 acc_train: 0.6598 acc_val: 0.7266\n",
      "Epoch: [142/1000] loss_train: 0.2175 acc_train: 0.6475 acc_val: 0.7266\n",
      "Epoch: [143/1000] loss_train: 0.2208 acc_train: 0.6414 acc_val: 0.7305\n",
      "Epoch: [144/1000] loss_train: 0.2143 acc_train: 0.6660 acc_val: 0.7363\n",
      "Epoch: [145/1000] loss_train: 0.2162 acc_train: 0.6434 acc_val: 0.7402\n",
      "Epoch: [146/1000] loss_train: 0.2156 acc_train: 0.6291 acc_val: 0.7441\n",
      "Epoch: [147/1000] loss_train: 0.2161 acc_train: 0.6475 acc_val: 0.7500\n",
      "Epoch: [148/1000] loss_train: 0.2105 acc_train: 0.6885 acc_val: 0.7578\n",
      "Epoch: [149/1000] loss_train: 0.2160 acc_train: 0.6537 acc_val: 0.7637\n",
      "Epoch: [150/1000] loss_train: 0.2089 acc_train: 0.6783 acc_val: 0.7695\n",
      "Epoch: [151/1000] loss_train: 0.2143 acc_train: 0.6680 acc_val: 0.7734\n",
      "Epoch: [152/1000] loss_train: 0.2140 acc_train: 0.6496 acc_val: 0.7812\n",
      "Epoch: [153/1000] loss_train: 0.2158 acc_train: 0.6803 acc_val: 0.7812\n",
      "Epoch: [154/1000] loss_train: 0.2171 acc_train: 0.6639 acc_val: 0.7871\n",
      "Epoch: [155/1000] loss_train: 0.2130 acc_train: 0.6762 acc_val: 0.7891\n",
      "Epoch: [156/1000] loss_train: 0.2127 acc_train: 0.6475 acc_val: 0.7930\n",
      "Epoch: [157/1000] loss_train: 0.2126 acc_train: 0.6824 acc_val: 0.7930\n",
      "Epoch: [158/1000] loss_train: 0.2054 acc_train: 0.6947 acc_val: 0.7988\n",
      "Epoch: [159/1000] loss_train: 0.2108 acc_train: 0.6824 acc_val: 0.7988\n",
      "Epoch: [160/1000] loss_train: 0.2125 acc_train: 0.6701 acc_val: 0.8027\n",
      "Epoch: [161/1000] loss_train: 0.2081 acc_train: 0.6906 acc_val: 0.8047\n",
      "Epoch: [162/1000] loss_train: 0.2069 acc_train: 0.6906 acc_val: 0.8125\n",
      "Epoch: [163/1000] loss_train: 0.2104 acc_train: 0.6803 acc_val: 0.8145\n",
      "Epoch: [164/1000] loss_train: 0.2060 acc_train: 0.7111 acc_val: 0.8105\n",
      "Epoch: [165/1000] loss_train: 0.2070 acc_train: 0.7193 acc_val: 0.8145\n",
      "Epoch: [166/1000] loss_train: 0.2111 acc_train: 0.6885 acc_val: 0.8145\n",
      "Epoch: [167/1000] loss_train: 0.2084 acc_train: 0.6885 acc_val: 0.8145\n",
      "Epoch: [168/1000] loss_train: 0.2065 acc_train: 0.6988 acc_val: 0.8164\n",
      "Epoch: [169/1000] loss_train: 0.2062 acc_train: 0.6885 acc_val: 0.8203\n",
      "Epoch: [170/1000] loss_train: 0.2069 acc_train: 0.7234 acc_val: 0.8262\n",
      "Epoch: [171/1000] loss_train: 0.2037 acc_train: 0.6906 acc_val: 0.8262\n",
      "Epoch: [172/1000] loss_train: 0.2054 acc_train: 0.7049 acc_val: 0.8262\n",
      "Epoch: [173/1000] loss_train: 0.2031 acc_train: 0.7234 acc_val: 0.8203\n",
      "Epoch: [174/1000] loss_train: 0.2066 acc_train: 0.6988 acc_val: 0.8184\n",
      "Epoch: [175/1000] loss_train: 0.2043 acc_train: 0.7234 acc_val: 0.8223\n",
      "Epoch: [176/1000] loss_train: 0.2017 acc_train: 0.7336 acc_val: 0.8223\n",
      "Epoch: [177/1000] loss_train: 0.2035 acc_train: 0.7213 acc_val: 0.8164\n",
      "Epoch: [178/1000] loss_train: 0.2023 acc_train: 0.7418 acc_val: 0.8145\n",
      "Epoch: [179/1000] loss_train: 0.1994 acc_train: 0.7623 acc_val: 0.8164\n",
      "Epoch: [180/1000] loss_train: 0.1994 acc_train: 0.7234 acc_val: 0.8164\n",
      "Epoch: [181/1000] loss_train: 0.2036 acc_train: 0.7049 acc_val: 0.8105\n",
      "Epoch: [182/1000] loss_train: 0.2057 acc_train: 0.6947 acc_val: 0.8047\n",
      "Epoch: [183/1000] loss_train: 0.2015 acc_train: 0.7234 acc_val: 0.8047\n",
      "Epoch: [184/1000] loss_train: 0.2013 acc_train: 0.7193 acc_val: 0.8066\n",
      "Epoch: [185/1000] loss_train: 0.1998 acc_train: 0.7234 acc_val: 0.8008\n",
      "Epoch: [186/1000] loss_train: 0.2017 acc_train: 0.7377 acc_val: 0.7969\n",
      "Epoch: [187/1000] loss_train: 0.2005 acc_train: 0.7049 acc_val: 0.7969\n",
      "Epoch: [188/1000] loss_train: 0.2000 acc_train: 0.7336 acc_val: 0.7871\n",
      "Epoch: [189/1000] loss_train: 0.1985 acc_train: 0.7234 acc_val: 0.7871\n",
      "Epoch: [190/1000] loss_train: 0.2003 acc_train: 0.7172 acc_val: 0.7852\n",
      "Epoch: [191/1000] loss_train: 0.2003 acc_train: 0.7234 acc_val: 0.7852\n",
      "Epoch: [192/1000] loss_train: 0.1972 acc_train: 0.7213 acc_val: 0.7773\n",
      "Epoch: [193/1000] loss_train: 0.2019 acc_train: 0.7111 acc_val: 0.7773\n",
      "Epoch: [194/1000] loss_train: 0.2016 acc_train: 0.6947 acc_val: 0.7754\n",
      "Epoch: [195/1000] loss_train: 0.2006 acc_train: 0.7111 acc_val: 0.7773\n",
      "Epoch: [196/1000] loss_train: 0.2044 acc_train: 0.6926 acc_val: 0.7754\n",
      "Epoch: [197/1000] loss_train: 0.2034 acc_train: 0.7295 acc_val: 0.7715\n",
      "Epoch: [198/1000] loss_train: 0.2022 acc_train: 0.6947 acc_val: 0.7773\n",
      "Epoch: [199/1000] loss_train: 0.2020 acc_train: 0.6947 acc_val: 0.7715\n",
      "Epoch: [200/1000] loss_train: 0.1970 acc_train: 0.7193 acc_val: 0.7715\n",
      "Epoch: [201/1000] loss_train: 0.2002 acc_train: 0.6926 acc_val: 0.7734\n",
      "Epoch: [202/1000] loss_train: 0.2001 acc_train: 0.6865 acc_val: 0.7773\n",
      "Epoch: [203/1000] loss_train: 0.1978 acc_train: 0.6885 acc_val: 0.7754\n",
      "Epoch: [204/1000] loss_train: 0.2023 acc_train: 0.7090 acc_val: 0.7734\n",
      "Epoch: [205/1000] loss_train: 0.2002 acc_train: 0.6783 acc_val: 0.7695\n",
      "Epoch: [206/1000] loss_train: 0.1987 acc_train: 0.6803 acc_val: 0.7695\n",
      "Epoch: [207/1000] loss_train: 0.2012 acc_train: 0.7029 acc_val: 0.7676\n",
      "Epoch: [208/1000] loss_train: 0.1981 acc_train: 0.7172 acc_val: 0.7676\n",
      "Epoch: [209/1000] loss_train: 0.1977 acc_train: 0.7008 acc_val: 0.7676\n",
      "Epoch: [210/1000] loss_train: 0.2028 acc_train: 0.6824 acc_val: 0.7676\n",
      "Epoch: [211/1000] loss_train: 0.2005 acc_train: 0.6906 acc_val: 0.7637\n",
      "Epoch: [212/1000] loss_train: 0.2024 acc_train: 0.6783 acc_val: 0.7637\n",
      "Epoch: [213/1000] loss_train: 0.1976 acc_train: 0.6988 acc_val: 0.7598\n",
      "Epoch: [214/1000] loss_train: 0.1962 acc_train: 0.6988 acc_val: 0.7598\n",
      "Epoch: [215/1000] loss_train: 0.1991 acc_train: 0.6844 acc_val: 0.7559\n",
      "Epoch: [216/1000] loss_train: 0.2007 acc_train: 0.6926 acc_val: 0.7539\n",
      "Epoch: [217/1000] loss_train: 0.2027 acc_train: 0.6824 acc_val: 0.7461\n",
      "Epoch: [218/1000] loss_train: 0.2031 acc_train: 0.6660 acc_val: 0.7441\n",
      "Epoch: [219/1000] loss_train: 0.2010 acc_train: 0.6660 acc_val: 0.7422\n",
      "Epoch: [220/1000] loss_train: 0.1995 acc_train: 0.7070 acc_val: 0.7383\n",
      "Epoch: [221/1000] loss_train: 0.2045 acc_train: 0.6721 acc_val: 0.7402\n",
      "Epoch: [222/1000] loss_train: 0.2023 acc_train: 0.6660 acc_val: 0.7383\n",
      "Epoch: [223/1000] loss_train: 0.2007 acc_train: 0.6680 acc_val: 0.7324\n",
      "Epoch: [224/1000] loss_train: 0.2050 acc_train: 0.6619 acc_val: 0.7324\n",
      "Epoch: [225/1000] loss_train: 0.2026 acc_train: 0.6742 acc_val: 0.7324\n",
      "Epoch: [226/1000] loss_train: 0.1982 acc_train: 0.6906 acc_val: 0.7305\n",
      "Epoch: [227/1000] loss_train: 0.1949 acc_train: 0.6762 acc_val: 0.7324\n",
      "Epoch: [228/1000] loss_train: 0.2030 acc_train: 0.6762 acc_val: 0.7344\n",
      "Epoch: [229/1000] loss_train: 0.2046 acc_train: 0.6639 acc_val: 0.7305\n",
      "Epoch: [230/1000] loss_train: 0.2031 acc_train: 0.6762 acc_val: 0.7305\n",
      "Epoch: [231/1000] loss_train: 0.2021 acc_train: 0.6783 acc_val: 0.7285\n",
      "Epoch: [232/1000] loss_train: 0.2002 acc_train: 0.6988 acc_val: 0.7266\n",
      "Epoch: [233/1000] loss_train: 0.2072 acc_train: 0.6578 acc_val: 0.7266\n",
      "Epoch: [234/1000] loss_train: 0.2049 acc_train: 0.6803 acc_val: 0.7246\n",
      "Epoch: [235/1000] loss_train: 0.2029 acc_train: 0.6578 acc_val: 0.7188\n",
      "Epoch: [236/1000] loss_train: 0.2056 acc_train: 0.6373 acc_val: 0.7188\n",
      "Epoch: [237/1000] loss_train: 0.1993 acc_train: 0.6844 acc_val: 0.7207\n",
      "Epoch: [238/1000] loss_train: 0.1991 acc_train: 0.6639 acc_val: 0.7129\n",
      "Epoch: [239/1000] loss_train: 0.2091 acc_train: 0.6619 acc_val: 0.7109\n",
      "Epoch: [240/1000] loss_train: 0.2024 acc_train: 0.6844 acc_val: 0.7129\n",
      "Epoch: [241/1000] loss_train: 0.2013 acc_train: 0.6721 acc_val: 0.7090\n",
      "Epoch: [242/1000] loss_train: 0.2027 acc_train: 0.6803 acc_val: 0.7070\n",
      "Epoch: [243/1000] loss_train: 0.2004 acc_train: 0.6844 acc_val: 0.7051\n",
      "Epoch: [244/1000] loss_train: 0.2040 acc_train: 0.6475 acc_val: 0.7051\n",
      "Epoch: [245/1000] loss_train: 0.2068 acc_train: 0.6455 acc_val: 0.7012\n",
      "Epoch: [246/1000] loss_train: 0.2015 acc_train: 0.6619 acc_val: 0.6992\n",
      "Epoch: [247/1000] loss_train: 0.2023 acc_train: 0.6783 acc_val: 0.6934\n",
      "Epoch: [248/1000] loss_train: 0.2004 acc_train: 0.6844 acc_val: 0.6934\n",
      "Epoch: [249/1000] loss_train: 0.1988 acc_train: 0.7029 acc_val: 0.6934\n",
      "Epoch: [250/1000] loss_train: 0.2000 acc_train: 0.6742 acc_val: 0.6934\n",
      "Epoch: [251/1000] loss_train: 0.1990 acc_train: 0.6824 acc_val: 0.6934\n",
      "Epoch: [252/1000] loss_train: 0.2039 acc_train: 0.6742 acc_val: 0.6934\n",
      "Epoch: [253/1000] loss_train: 0.2049 acc_train: 0.6537 acc_val: 0.6934\n",
      "Epoch: [254/1000] loss_train: 0.1978 acc_train: 0.6844 acc_val: 0.6953\n",
      "Epoch: [255/1000] loss_train: 0.2064 acc_train: 0.6701 acc_val: 0.6992\n",
      "Epoch: [256/1000] loss_train: 0.2139 acc_train: 0.6660 acc_val: 0.6992\n",
      "Epoch: [257/1000] loss_train: 0.2038 acc_train: 0.6496 acc_val: 0.7031\n",
      "Epoch: [258/1000] loss_train: 0.2017 acc_train: 0.6639 acc_val: 0.7051\n",
      "Epoch: [259/1000] loss_train: 0.2025 acc_train: 0.6680 acc_val: 0.7070\n",
      "Epoch: [260/1000] loss_train: 0.2023 acc_train: 0.6639 acc_val: 0.7129\n",
      "Epoch: [261/1000] loss_train: 0.2094 acc_train: 0.6393 acc_val: 0.7109\n",
      "Epoch: [262/1000] loss_train: 0.2061 acc_train: 0.6516 acc_val: 0.7129\n",
      "Epoch: [263/1000] loss_train: 0.2000 acc_train: 0.6783 acc_val: 0.7148\n",
      "Epoch: [264/1000] loss_train: 0.1974 acc_train: 0.6680 acc_val: 0.7188\n",
      "Epoch: [265/1000] loss_train: 0.2099 acc_train: 0.6475 acc_val: 0.7188\n",
      "Epoch: [266/1000] loss_train: 0.2000 acc_train: 0.6721 acc_val: 0.7188\n",
      "Epoch: [267/1000] loss_train: 0.2085 acc_train: 0.6537 acc_val: 0.7227\n",
      "Epoch: [268/1000] loss_train: 0.1991 acc_train: 0.6516 acc_val: 0.7246\n",
      "Epoch: [269/1000] loss_train: 0.2000 acc_train: 0.6803 acc_val: 0.7246\n",
      "Epoch: [270/1000] loss_train: 0.1995 acc_train: 0.6783 acc_val: 0.7285\n",
      "Epoch: [271/1000] loss_train: 0.1964 acc_train: 0.6660 acc_val: 0.7305\n",
      "Epoch: [272/1000] loss_train: 0.1953 acc_train: 0.6844 acc_val: 0.7305\n",
      "Epoch: [273/1000] loss_train: 0.2009 acc_train: 0.6844 acc_val: 0.7305\n",
      "Epoch: [274/1000] loss_train: 0.1957 acc_train: 0.6988 acc_val: 0.7324\n",
      "Epoch: [275/1000] loss_train: 0.1951 acc_train: 0.6844 acc_val: 0.7324\n",
      "Epoch: [276/1000] loss_train: 0.1956 acc_train: 0.6783 acc_val: 0.7324\n",
      "Epoch: [277/1000] loss_train: 0.1928 acc_train: 0.7131 acc_val: 0.7344\n",
      "Epoch: [278/1000] loss_train: 0.1938 acc_train: 0.6783 acc_val: 0.7324\n",
      "Epoch: [279/1000] loss_train: 0.1937 acc_train: 0.6783 acc_val: 0.7344\n",
      "Epoch: [280/1000] loss_train: 0.1968 acc_train: 0.7008 acc_val: 0.7363\n",
      "Epoch: [281/1000] loss_train: 0.1987 acc_train: 0.6721 acc_val: 0.7363\n",
      "Epoch: [282/1000] loss_train: 0.1916 acc_train: 0.6885 acc_val: 0.7363\n",
      "Epoch: [283/1000] loss_train: 0.2019 acc_train: 0.6680 acc_val: 0.7383\n",
      "Epoch: [284/1000] loss_train: 0.2019 acc_train: 0.6619 acc_val: 0.7422\n",
      "Epoch: [285/1000] loss_train: 0.1986 acc_train: 0.6783 acc_val: 0.7461\n",
      "Epoch: [286/1000] loss_train: 0.1959 acc_train: 0.6865 acc_val: 0.7480\n",
      "Epoch: [287/1000] loss_train: 0.1958 acc_train: 0.6824 acc_val: 0.7539\n",
      "Epoch: [288/1000] loss_train: 0.1981 acc_train: 0.6742 acc_val: 0.7559\n",
      "Epoch: [289/1000] loss_train: 0.1889 acc_train: 0.7336 acc_val: 0.7539\n",
      "Epoch: [290/1000] loss_train: 0.1932 acc_train: 0.6762 acc_val: 0.7676\n",
      "Epoch: [291/1000] loss_train: 0.1919 acc_train: 0.6967 acc_val: 0.7676\n",
      "Epoch: [292/1000] loss_train: 0.1977 acc_train: 0.6803 acc_val: 0.7676\n",
      "Epoch: [293/1000] loss_train: 0.1975 acc_train: 0.6885 acc_val: 0.7676\n",
      "Epoch: [294/1000] loss_train: 0.1948 acc_train: 0.6967 acc_val: 0.7715\n",
      "Epoch: [295/1000] loss_train: 0.1911 acc_train: 0.7131 acc_val: 0.7695\n",
      "Epoch: [296/1000] loss_train: 0.1912 acc_train: 0.7029 acc_val: 0.7676\n",
      "Epoch: [297/1000] loss_train: 0.1947 acc_train: 0.6906 acc_val: 0.7695\n",
      "Epoch: [298/1000] loss_train: 0.1971 acc_train: 0.6926 acc_val: 0.7676\n",
      "Epoch: [299/1000] loss_train: 0.1939 acc_train: 0.6906 acc_val: 0.7676\n",
      "Epoch: [300/1000] loss_train: 0.1931 acc_train: 0.7008 acc_val: 0.7676\n",
      "Epoch: [301/1000] loss_train: 0.1921 acc_train: 0.7111 acc_val: 0.7676\n",
      "Epoch: [302/1000] loss_train: 0.1954 acc_train: 0.6967 acc_val: 0.7695\n",
      "Epoch: [303/1000] loss_train: 0.1889 acc_train: 0.7213 acc_val: 0.7715\n",
      "Epoch: [304/1000] loss_train: 0.1937 acc_train: 0.6947 acc_val: 0.7773\n",
      "Epoch: [305/1000] loss_train: 0.1852 acc_train: 0.7275 acc_val: 0.7773\n",
      "Epoch: [306/1000] loss_train: 0.1940 acc_train: 0.7172 acc_val: 0.7754\n",
      "Epoch: [307/1000] loss_train: 0.1928 acc_train: 0.6701 acc_val: 0.7734\n",
      "Epoch: [308/1000] loss_train: 0.1914 acc_train: 0.7295 acc_val: 0.7715\n",
      "Epoch: [309/1000] loss_train: 0.1911 acc_train: 0.7152 acc_val: 0.7715\n",
      "Epoch: [310/1000] loss_train: 0.1909 acc_train: 0.7152 acc_val: 0.7734\n",
      "Epoch: [311/1000] loss_train: 0.1799 acc_train: 0.7357 acc_val: 0.7715\n",
      "Epoch: [312/1000] loss_train: 0.1874 acc_train: 0.7172 acc_val: 0.7734\n",
      "Epoch: [313/1000] loss_train: 0.1879 acc_train: 0.7070 acc_val: 0.7734\n",
      "Epoch: [314/1000] loss_train: 0.1925 acc_train: 0.7090 acc_val: 0.7734\n",
      "Epoch: [315/1000] loss_train: 0.1812 acc_train: 0.7480 acc_val: 0.7734\n",
      "Epoch: [316/1000] loss_train: 0.1870 acc_train: 0.7398 acc_val: 0.7754\n",
      "Epoch: [317/1000] loss_train: 0.1904 acc_train: 0.7111 acc_val: 0.7754\n",
      "Epoch: [318/1000] loss_train: 0.1824 acc_train: 0.7295 acc_val: 0.7754\n",
      "Epoch: [319/1000] loss_train: 0.1885 acc_train: 0.7008 acc_val: 0.7773\n",
      "Epoch: [320/1000] loss_train: 0.1873 acc_train: 0.7090 acc_val: 0.7773\n",
      "Epoch: [321/1000] loss_train: 0.1845 acc_train: 0.7336 acc_val: 0.7793\n",
      "Epoch: [322/1000] loss_train: 0.1838 acc_train: 0.7480 acc_val: 0.7793\n",
      "Epoch: [323/1000] loss_train: 0.1886 acc_train: 0.7234 acc_val: 0.7793\n",
      "Epoch: [324/1000] loss_train: 0.1833 acc_train: 0.7541 acc_val: 0.7793\n",
      "Epoch: [325/1000] loss_train: 0.1859 acc_train: 0.7398 acc_val: 0.7793\n",
      "Epoch: [326/1000] loss_train: 0.1851 acc_train: 0.7520 acc_val: 0.7793\n",
      "Epoch: [327/1000] loss_train: 0.1898 acc_train: 0.7111 acc_val: 0.7812\n",
      "Epoch: [328/1000] loss_train: 0.1838 acc_train: 0.7418 acc_val: 0.7832\n",
      "Epoch: [329/1000] loss_train: 0.1866 acc_train: 0.7336 acc_val: 0.7832\n",
      "Epoch: [330/1000] loss_train: 0.1852 acc_train: 0.7357 acc_val: 0.7871\n",
      "Epoch: [331/1000] loss_train: 0.1849 acc_train: 0.7418 acc_val: 0.7891\n",
      "Epoch: [332/1000] loss_train: 0.1827 acc_train: 0.7480 acc_val: 0.7891\n",
      "Epoch: [333/1000] loss_train: 0.1785 acc_train: 0.7480 acc_val: 0.7910\n",
      "Epoch: [334/1000] loss_train: 0.1842 acc_train: 0.7295 acc_val: 0.7891\n",
      "Epoch: [335/1000] loss_train: 0.1775 acc_train: 0.7664 acc_val: 0.7930\n",
      "Epoch: [336/1000] loss_train: 0.1829 acc_train: 0.7336 acc_val: 0.7969\n",
      "Epoch: [337/1000] loss_train: 0.1841 acc_train: 0.7480 acc_val: 0.7969\n",
      "Epoch: [338/1000] loss_train: 0.1790 acc_train: 0.7766 acc_val: 0.7949\n",
      "Epoch: [339/1000] loss_train: 0.1831 acc_train: 0.7500 acc_val: 0.7969\n",
      "Epoch: [340/1000] loss_train: 0.1811 acc_train: 0.7500 acc_val: 0.8027\n",
      "Epoch: [341/1000] loss_train: 0.1819 acc_train: 0.7316 acc_val: 0.8086\n",
      "Epoch: [342/1000] loss_train: 0.1875 acc_train: 0.7254 acc_val: 0.8105\n",
      "Epoch: [343/1000] loss_train: 0.1808 acc_train: 0.7561 acc_val: 0.8105\n",
      "Epoch: [344/1000] loss_train: 0.1812 acc_train: 0.7500 acc_val: 0.8125\n",
      "Epoch: [345/1000] loss_train: 0.1821 acc_train: 0.7602 acc_val: 0.8125\n",
      "Epoch: [346/1000] loss_train: 0.1849 acc_train: 0.7418 acc_val: 0.8125\n",
      "Epoch: [347/1000] loss_train: 0.1832 acc_train: 0.7664 acc_val: 0.8145\n",
      "Epoch: [348/1000] loss_train: 0.1842 acc_train: 0.7377 acc_val: 0.8086\n",
      "Epoch: [349/1000] loss_train: 0.1836 acc_train: 0.7500 acc_val: 0.8105\n",
      "Epoch: [350/1000] loss_train: 0.1808 acc_train: 0.7582 acc_val: 0.8105\n",
      "Epoch: [351/1000] loss_train: 0.1825 acc_train: 0.7418 acc_val: 0.8105\n",
      "Epoch: [352/1000] loss_train: 0.1782 acc_train: 0.7828 acc_val: 0.8125\n",
      "Epoch: [353/1000] loss_train: 0.1859 acc_train: 0.7254 acc_val: 0.8145\n",
      "Epoch: [354/1000] loss_train: 0.1776 acc_train: 0.7664 acc_val: 0.8145\n",
      "Epoch: [355/1000] loss_train: 0.1824 acc_train: 0.7541 acc_val: 0.8164\n",
      "Epoch: [356/1000] loss_train: 0.1776 acc_train: 0.7602 acc_val: 0.8164\n",
      "Epoch: [357/1000] loss_train: 0.1781 acc_train: 0.7602 acc_val: 0.8164\n",
      "Epoch: [358/1000] loss_train: 0.1786 acc_train: 0.7643 acc_val: 0.8184\n",
      "Epoch: [359/1000] loss_train: 0.1869 acc_train: 0.7357 acc_val: 0.8184\n",
      "Epoch: [360/1000] loss_train: 0.1854 acc_train: 0.7561 acc_val: 0.8203\n",
      "Epoch: [361/1000] loss_train: 0.1856 acc_train: 0.7582 acc_val: 0.8184\n",
      "Epoch: [362/1000] loss_train: 0.1806 acc_train: 0.7541 acc_val: 0.8203\n",
      "Epoch: [363/1000] loss_train: 0.1823 acc_train: 0.7377 acc_val: 0.8223\n",
      "Epoch: [364/1000] loss_train: 0.1804 acc_train: 0.7500 acc_val: 0.8223\n",
      "Epoch: [365/1000] loss_train: 0.1801 acc_train: 0.7520 acc_val: 0.8223\n",
      "Epoch: [366/1000] loss_train: 0.1755 acc_train: 0.7684 acc_val: 0.8203\n",
      "Epoch: [367/1000] loss_train: 0.1796 acc_train: 0.7541 acc_val: 0.8223\n",
      "Epoch: [368/1000] loss_train: 0.1816 acc_train: 0.7377 acc_val: 0.8242\n",
      "Epoch: [369/1000] loss_train: 0.1777 acc_train: 0.7787 acc_val: 0.8242\n",
      "Epoch: [370/1000] loss_train: 0.1801 acc_train: 0.7439 acc_val: 0.8281\n",
      "Acc_best is updated to 0.8281. Model checkpoint is saved to ./model/BA1k_MVC_Student.pt\n",
      "Test accuracy is 0.8110\n",
      "Epoch: [371/1000] loss_train: 0.1785 acc_train: 0.7480 acc_val: 0.8281\n",
      "Epoch: [372/1000] loss_train: 0.1823 acc_train: 0.7664 acc_val: 0.8320\n",
      "Acc_best is updated to 0.8320. Model checkpoint is saved to ./model/BA1k_MVC_Student.pt\n",
      "Test accuracy is 0.8120\n",
      "Epoch: [373/1000] loss_train: 0.1865 acc_train: 0.7295 acc_val: 0.8301\n",
      "Epoch: [374/1000] loss_train: 0.1832 acc_train: 0.7582 acc_val: 0.8301\n",
      "Epoch: [375/1000] loss_train: 0.1874 acc_train: 0.7561 acc_val: 0.8320\n",
      "Epoch: [376/1000] loss_train: 0.1798 acc_train: 0.7480 acc_val: 0.8340\n",
      "Acc_best is updated to 0.8340. Model checkpoint is saved to ./model/BA1k_MVC_Student.pt\n",
      "Test accuracy is 0.8110\n",
      "Epoch: [377/1000] loss_train: 0.1794 acc_train: 0.7746 acc_val: 0.8320\n",
      "Epoch: [378/1000] loss_train: 0.1820 acc_train: 0.7398 acc_val: 0.8320\n",
      "Epoch: [379/1000] loss_train: 0.1861 acc_train: 0.7480 acc_val: 0.8301\n",
      "Epoch: [380/1000] loss_train: 0.1780 acc_train: 0.7910 acc_val: 0.8320\n",
      "Epoch: [381/1000] loss_train: 0.1799 acc_train: 0.7541 acc_val: 0.8320\n",
      "Epoch: [382/1000] loss_train: 0.1779 acc_train: 0.7295 acc_val: 0.8301\n",
      "Epoch: [383/1000] loss_train: 0.1863 acc_train: 0.7398 acc_val: 0.8340\n",
      "Epoch: [384/1000] loss_train: 0.1806 acc_train: 0.7520 acc_val: 0.8340\n",
      "Epoch: [385/1000] loss_train: 0.1789 acc_train: 0.7664 acc_val: 0.8359\n",
      "Acc_best is updated to 0.8359. Model checkpoint is saved to ./model/BA1k_MVC_Student.pt\n",
      "Test accuracy is 0.8090\n",
      "Epoch: [386/1000] loss_train: 0.1785 acc_train: 0.7746 acc_val: 0.8359\n",
      "Epoch: [387/1000] loss_train: 0.1800 acc_train: 0.7684 acc_val: 0.8359\n",
      "Epoch: [388/1000] loss_train: 0.1784 acc_train: 0.7643 acc_val: 0.8359\n",
      "Epoch: [389/1000] loss_train: 0.1842 acc_train: 0.7520 acc_val: 0.8340\n",
      "Epoch: [390/1000] loss_train: 0.1863 acc_train: 0.7520 acc_val: 0.8340\n",
      "Epoch: [391/1000] loss_train: 0.1849 acc_train: 0.7295 acc_val: 0.8340\n",
      "Epoch: [392/1000] loss_train: 0.1838 acc_train: 0.7520 acc_val: 0.8340\n",
      "Epoch: [393/1000] loss_train: 0.1849 acc_train: 0.7746 acc_val: 0.8359\n",
      "Epoch: [394/1000] loss_train: 0.1779 acc_train: 0.7623 acc_val: 0.8359\n",
      "Epoch: [395/1000] loss_train: 0.1827 acc_train: 0.7684 acc_val: 0.8359\n",
      "Epoch: [396/1000] loss_train: 0.1787 acc_train: 0.7602 acc_val: 0.8359\n",
      "Epoch: [397/1000] loss_train: 0.1825 acc_train: 0.7561 acc_val: 0.8359\n",
      "Epoch: [398/1000] loss_train: 0.1804 acc_train: 0.7480 acc_val: 0.8359\n",
      "Epoch: [399/1000] loss_train: 0.1769 acc_train: 0.7746 acc_val: 0.8359\n",
      "Epoch: [400/1000] loss_train: 0.1835 acc_train: 0.7459 acc_val: 0.8359\n",
      "Epoch: [401/1000] loss_train: 0.1883 acc_train: 0.7336 acc_val: 0.8359\n",
      "Epoch: [402/1000] loss_train: 0.1835 acc_train: 0.7500 acc_val: 0.8359\n",
      "Epoch: [403/1000] loss_train: 0.1781 acc_train: 0.7500 acc_val: 0.8359\n",
      "Epoch: [404/1000] loss_train: 0.1790 acc_train: 0.7582 acc_val: 0.8359\n",
      "Epoch: [405/1000] loss_train: 0.1845 acc_train: 0.7418 acc_val: 0.8359\n",
      "Epoch: [406/1000] loss_train: 0.1841 acc_train: 0.7377 acc_val: 0.8359\n",
      "Epoch: [407/1000] loss_train: 0.1765 acc_train: 0.7787 acc_val: 0.8359\n",
      "Epoch: [408/1000] loss_train: 0.1802 acc_train: 0.7623 acc_val: 0.8359\n",
      "Epoch: [409/1000] loss_train: 0.1782 acc_train: 0.7725 acc_val: 0.8359\n",
      "Epoch: [410/1000] loss_train: 0.1866 acc_train: 0.7541 acc_val: 0.8359\n",
      "Epoch: [411/1000] loss_train: 0.1777 acc_train: 0.7643 acc_val: 0.8359\n",
      "Epoch: [412/1000] loss_train: 0.1807 acc_train: 0.7377 acc_val: 0.8359\n",
      "Epoch: [413/1000] loss_train: 0.1798 acc_train: 0.7254 acc_val: 0.8359\n",
      "Epoch: [414/1000] loss_train: 0.1820 acc_train: 0.7664 acc_val: 0.8359\n",
      "Epoch: [415/1000] loss_train: 0.1777 acc_train: 0.7561 acc_val: 0.8359\n",
      "Epoch: [416/1000] loss_train: 0.1792 acc_train: 0.7520 acc_val: 0.8359\n",
      "Epoch: [417/1000] loss_train: 0.1806 acc_train: 0.7480 acc_val: 0.8359\n",
      "Epoch: [418/1000] loss_train: 0.1809 acc_train: 0.7418 acc_val: 0.8359\n",
      "Epoch: [419/1000] loss_train: 0.1796 acc_train: 0.7623 acc_val: 0.8359\n",
      "Epoch: [420/1000] loss_train: 0.1780 acc_train: 0.7541 acc_val: 0.8359\n",
      "Epoch: [421/1000] loss_train: 0.1812 acc_train: 0.7459 acc_val: 0.8359\n",
      "Epoch: [422/1000] loss_train: 0.1725 acc_train: 0.7684 acc_val: 0.8340\n",
      "Epoch: [423/1000] loss_train: 0.1798 acc_train: 0.7520 acc_val: 0.8340\n",
      "Epoch: [424/1000] loss_train: 0.1796 acc_train: 0.7684 acc_val: 0.8340\n",
      "Epoch: [425/1000] loss_train: 0.1817 acc_train: 0.7234 acc_val: 0.8340\n",
      "Epoch: [426/1000] loss_train: 0.1769 acc_train: 0.7439 acc_val: 0.8340\n",
      "Epoch: [427/1000] loss_train: 0.1748 acc_train: 0.7623 acc_val: 0.8340\n",
      "Epoch: [428/1000] loss_train: 0.1750 acc_train: 0.7602 acc_val: 0.8359\n",
      "Epoch: [429/1000] loss_train: 0.1739 acc_train: 0.7582 acc_val: 0.8340\n",
      "Epoch: [430/1000] loss_train: 0.1742 acc_train: 0.7541 acc_val: 0.8340\n",
      "Epoch: [431/1000] loss_train: 0.1745 acc_train: 0.7500 acc_val: 0.8359\n",
      "Epoch: [432/1000] loss_train: 0.1701 acc_train: 0.7664 acc_val: 0.8359\n",
      "Epoch: [433/1000] loss_train: 0.1759 acc_train: 0.7439 acc_val: 0.8359\n",
      "Epoch: [434/1000] loss_train: 0.1784 acc_train: 0.7316 acc_val: 0.8359\n",
      "Epoch: [435/1000] loss_train: 0.1756 acc_train: 0.7582 acc_val: 0.8340\n",
      "Epoch: [436/1000] loss_train: 0.1738 acc_train: 0.7602 acc_val: 0.8320\n",
      "Epoch: [437/1000] loss_train: 0.1751 acc_train: 0.7459 acc_val: 0.8340\n",
      "Epoch: [438/1000] loss_train: 0.1727 acc_train: 0.7807 acc_val: 0.8359\n",
      "Epoch: [439/1000] loss_train: 0.1801 acc_train: 0.7398 acc_val: 0.8320\n",
      "Epoch: [440/1000] loss_train: 0.1781 acc_train: 0.7480 acc_val: 0.8320\n",
      "Epoch: [441/1000] loss_train: 0.1769 acc_train: 0.7418 acc_val: 0.8340\n",
      "Epoch: [442/1000] loss_train: 0.1768 acc_train: 0.7500 acc_val: 0.8340\n",
      "Epoch: [443/1000] loss_train: 0.1796 acc_train: 0.7582 acc_val: 0.8320\n",
      "Epoch: [444/1000] loss_train: 0.1707 acc_train: 0.8033 acc_val: 0.8320\n",
      "Epoch: [445/1000] loss_train: 0.1731 acc_train: 0.7602 acc_val: 0.8320\n",
      "Epoch: [446/1000] loss_train: 0.1739 acc_train: 0.7643 acc_val: 0.8320\n",
      "Epoch: [447/1000] loss_train: 0.1780 acc_train: 0.7418 acc_val: 0.8340\n",
      "Epoch: [448/1000] loss_train: 0.1715 acc_train: 0.7520 acc_val: 0.8340\n",
      "Epoch: [449/1000] loss_train: 0.1758 acc_train: 0.7684 acc_val: 0.8340\n",
      "Epoch: [450/1000] loss_train: 0.1728 acc_train: 0.7930 acc_val: 0.8340\n",
      "Epoch: [451/1000] loss_train: 0.1750 acc_train: 0.7787 acc_val: 0.8359\n",
      "Epoch: [452/1000] loss_train: 0.1775 acc_train: 0.7623 acc_val: 0.8340\n",
      "Epoch: [453/1000] loss_train: 0.1753 acc_train: 0.7418 acc_val: 0.8320\n",
      "Epoch: [454/1000] loss_train: 0.1672 acc_train: 0.7705 acc_val: 0.8320\n",
      "Epoch: [455/1000] loss_train: 0.1732 acc_train: 0.7725 acc_val: 0.8320\n",
      "Epoch: [456/1000] loss_train: 0.1687 acc_train: 0.7480 acc_val: 0.8320\n",
      "Epoch: [457/1000] loss_train: 0.1735 acc_train: 0.7766 acc_val: 0.8320\n",
      "Epoch: [458/1000] loss_train: 0.1726 acc_train: 0.7725 acc_val: 0.8281\n",
      "Epoch: [459/1000] loss_train: 0.1759 acc_train: 0.7418 acc_val: 0.8262\n",
      "Epoch: [460/1000] loss_train: 0.1665 acc_train: 0.7746 acc_val: 0.8262\n",
      "Epoch: [461/1000] loss_train: 0.1749 acc_train: 0.7623 acc_val: 0.8281\n",
      "Epoch: [462/1000] loss_train: 0.1683 acc_train: 0.7807 acc_val: 0.8281\n",
      "Epoch: [463/1000] loss_train: 0.1677 acc_train: 0.7869 acc_val: 0.8281\n",
      "Epoch: [464/1000] loss_train: 0.1740 acc_train: 0.7418 acc_val: 0.8281\n",
      "Epoch: [465/1000] loss_train: 0.1708 acc_train: 0.7725 acc_val: 0.8262\n",
      "Epoch: [466/1000] loss_train: 0.1663 acc_train: 0.7889 acc_val: 0.8262\n",
      "Epoch: [467/1000] loss_train: 0.1682 acc_train: 0.7418 acc_val: 0.8242\n",
      "Epoch: [468/1000] loss_train: 0.1669 acc_train: 0.7766 acc_val: 0.8223\n",
      "Epoch: [469/1000] loss_train: 0.1690 acc_train: 0.7582 acc_val: 0.8223\n",
      "Epoch: [470/1000] loss_train: 0.1708 acc_train: 0.7561 acc_val: 0.8223\n",
      "Epoch: [471/1000] loss_train: 0.1708 acc_train: 0.7561 acc_val: 0.8223\n",
      "Epoch: [472/1000] loss_train: 0.1632 acc_train: 0.7930 acc_val: 0.8223\n",
      "Epoch: [473/1000] loss_train: 0.1690 acc_train: 0.7561 acc_val: 0.8223\n",
      "Epoch: [474/1000] loss_train: 0.1735 acc_train: 0.7520 acc_val: 0.8203\n",
      "Epoch: [475/1000] loss_train: 0.1668 acc_train: 0.7848 acc_val: 0.8223\n",
      "Epoch: [476/1000] loss_train: 0.1718 acc_train: 0.7541 acc_val: 0.8223\n",
      "Epoch: [477/1000] loss_train: 0.1645 acc_train: 0.7910 acc_val: 0.8203\n",
      "Epoch: [478/1000] loss_train: 0.1630 acc_train: 0.7807 acc_val: 0.8203\n",
      "Epoch: [479/1000] loss_train: 0.1681 acc_train: 0.7520 acc_val: 0.8203\n",
      "Epoch: [480/1000] loss_train: 0.1629 acc_train: 0.7766 acc_val: 0.8223\n",
      "Epoch: [481/1000] loss_train: 0.1689 acc_train: 0.7623 acc_val: 0.8223\n",
      "Epoch: [482/1000] loss_train: 0.1687 acc_train: 0.7582 acc_val: 0.8223\n",
      "Epoch: [483/1000] loss_train: 0.1741 acc_train: 0.7582 acc_val: 0.8242\n",
      "Epoch: [484/1000] loss_train: 0.1672 acc_train: 0.7705 acc_val: 0.8223\n",
      "Epoch: [485/1000] loss_train: 0.1687 acc_train: 0.7643 acc_val: 0.8223\n",
      "Epoch: [486/1000] loss_train: 0.1695 acc_train: 0.7684 acc_val: 0.8184\n",
      "Epoch: [487/1000] loss_train: 0.1666 acc_train: 0.7787 acc_val: 0.8184\n",
      "Epoch: [488/1000] loss_train: 0.1657 acc_train: 0.7684 acc_val: 0.8184\n",
      "Epoch: [489/1000] loss_train: 0.1746 acc_train: 0.7480 acc_val: 0.8203\n",
      "Epoch: [490/1000] loss_train: 0.1677 acc_train: 0.7869 acc_val: 0.8164\n",
      "Epoch: [491/1000] loss_train: 0.1666 acc_train: 0.7848 acc_val: 0.8184\n",
      "Epoch: [492/1000] loss_train: 0.1715 acc_train: 0.7643 acc_val: 0.8184\n",
      "Epoch: [493/1000] loss_train: 0.1712 acc_train: 0.7725 acc_val: 0.8164\n",
      "Epoch: [494/1000] loss_train: 0.1671 acc_train: 0.7480 acc_val: 0.8145\n",
      "Epoch: [495/1000] loss_train: 0.1774 acc_train: 0.7336 acc_val: 0.8145\n",
      "Epoch: [496/1000] loss_train: 0.1719 acc_train: 0.7705 acc_val: 0.8145\n",
      "Epoch: [497/1000] loss_train: 0.1641 acc_train: 0.7889 acc_val: 0.8145\n",
      "Epoch: [498/1000] loss_train: 0.1672 acc_train: 0.7869 acc_val: 0.8145\n",
      "Epoch: [499/1000] loss_train: 0.1683 acc_train: 0.7602 acc_val: 0.8145\n",
      "Epoch: [500/1000] loss_train: 0.1764 acc_train: 0.7582 acc_val: 0.8125\n",
      "Epoch: [501/1000] loss_train: 0.1739 acc_train: 0.7787 acc_val: 0.8125\n",
      "Epoch: [502/1000] loss_train: 0.1700 acc_train: 0.7643 acc_val: 0.8086\n",
      "Epoch: [503/1000] loss_train: 0.1693 acc_train: 0.7561 acc_val: 0.8086\n",
      "Epoch: [504/1000] loss_train: 0.1695 acc_train: 0.7684 acc_val: 0.8086\n",
      "Epoch: [505/1000] loss_train: 0.1655 acc_train: 0.7705 acc_val: 0.8086\n",
      "Epoch: [506/1000] loss_train: 0.1704 acc_train: 0.7787 acc_val: 0.8105\n",
      "Epoch: [507/1000] loss_train: 0.1670 acc_train: 0.7664 acc_val: 0.8086\n",
      "Epoch: [508/1000] loss_train: 0.1701 acc_train: 0.7643 acc_val: 0.8086\n",
      "Epoch: [509/1000] loss_train: 0.1663 acc_train: 0.7623 acc_val: 0.8086\n",
      "Epoch: [510/1000] loss_train: 0.1692 acc_train: 0.7582 acc_val: 0.8086\n",
      "Epoch: [511/1000] loss_train: 0.1786 acc_train: 0.7418 acc_val: 0.8086\n",
      "Epoch: [512/1000] loss_train: 0.1657 acc_train: 0.7889 acc_val: 0.8086\n",
      "Epoch: [513/1000] loss_train: 0.1740 acc_train: 0.7582 acc_val: 0.8086\n",
      "Epoch: [514/1000] loss_train: 0.1705 acc_train: 0.7684 acc_val: 0.8086\n",
      "Epoch: [515/1000] loss_train: 0.1673 acc_train: 0.7664 acc_val: 0.8066\n",
      "Epoch: [516/1000] loss_train: 0.1715 acc_train: 0.7602 acc_val: 0.8066\n",
      "Epoch: [517/1000] loss_train: 0.1735 acc_train: 0.7582 acc_val: 0.8066\n",
      "Epoch: [518/1000] loss_train: 0.1806 acc_train: 0.7520 acc_val: 0.8047\n",
      "Epoch: [519/1000] loss_train: 0.1707 acc_train: 0.7684 acc_val: 0.8047\n",
      "Epoch: [520/1000] loss_train: 0.1704 acc_train: 0.7828 acc_val: 0.8027\n",
      "Epoch: [521/1000] loss_train: 0.1709 acc_train: 0.7725 acc_val: 0.8008\n",
      "Epoch: [522/1000] loss_train: 0.1662 acc_train: 0.7889 acc_val: 0.7969\n",
      "Epoch: [523/1000] loss_train: 0.1677 acc_train: 0.7664 acc_val: 0.7969\n",
      "Epoch: [524/1000] loss_train: 0.1769 acc_train: 0.7520 acc_val: 0.7969\n",
      "Epoch: [525/1000] loss_train: 0.1685 acc_train: 0.7766 acc_val: 0.7969\n",
      "Epoch: [526/1000] loss_train: 0.1718 acc_train: 0.7582 acc_val: 0.7988\n",
      "Epoch: [527/1000] loss_train: 0.1716 acc_train: 0.7623 acc_val: 0.7988\n",
      "Epoch: [528/1000] loss_train: 0.1618 acc_train: 0.8156 acc_val: 0.7988\n",
      "Epoch: [529/1000] loss_train: 0.1649 acc_train: 0.7787 acc_val: 0.7988\n",
      "Epoch: [530/1000] loss_train: 0.1662 acc_train: 0.7869 acc_val: 0.7988\n",
      "Epoch: [531/1000] loss_train: 0.1755 acc_train: 0.7705 acc_val: 0.7988\n",
      "Epoch: [532/1000] loss_train: 0.1718 acc_train: 0.7398 acc_val: 0.7988\n",
      "Epoch: [533/1000] loss_train: 0.1700 acc_train: 0.7664 acc_val: 0.7969\n",
      "Epoch: [534/1000] loss_train: 0.1725 acc_train: 0.7725 acc_val: 0.7969\n",
      "Epoch: [535/1000] loss_train: 0.1699 acc_train: 0.7766 acc_val: 0.7949\n",
      "Epoch: [536/1000] loss_train: 0.1692 acc_train: 0.7623 acc_val: 0.7949\n",
      "Epoch: [537/1000] loss_train: 0.1727 acc_train: 0.7766 acc_val: 0.7930\n",
      "Epoch: [538/1000] loss_train: 0.1750 acc_train: 0.7623 acc_val: 0.7930\n",
      "Epoch: [539/1000] loss_train: 0.1653 acc_train: 0.7930 acc_val: 0.7930\n",
      "Epoch: [540/1000] loss_train: 0.1791 acc_train: 0.7500 acc_val: 0.7930\n",
      "Epoch: [541/1000] loss_train: 0.1751 acc_train: 0.7705 acc_val: 0.7930\n",
      "Epoch: [542/1000] loss_train: 0.1717 acc_train: 0.7705 acc_val: 0.7930\n",
      "Epoch: [543/1000] loss_train: 0.1709 acc_train: 0.7602 acc_val: 0.7930\n",
      "Epoch: [544/1000] loss_train: 0.1735 acc_train: 0.7643 acc_val: 0.7930\n",
      "Epoch: [545/1000] loss_train: 0.1723 acc_train: 0.7725 acc_val: 0.7930\n",
      "Epoch: [546/1000] loss_train: 0.1782 acc_train: 0.7439 acc_val: 0.7930\n",
      "Epoch: [547/1000] loss_train: 0.1762 acc_train: 0.7705 acc_val: 0.7949\n",
      "Epoch: [548/1000] loss_train: 0.1702 acc_train: 0.7848 acc_val: 0.7949\n",
      "Epoch: [549/1000] loss_train: 0.1705 acc_train: 0.7664 acc_val: 0.7949\n",
      "Epoch: [550/1000] loss_train: 0.1676 acc_train: 0.7602 acc_val: 0.7949\n",
      "Epoch: [551/1000] loss_train: 0.1764 acc_train: 0.7643 acc_val: 0.7949\n",
      "Epoch: [552/1000] loss_train: 0.1766 acc_train: 0.7459 acc_val: 0.7949\n",
      "Epoch: [553/1000] loss_train: 0.1652 acc_train: 0.7664 acc_val: 0.7949\n",
      "Epoch: [554/1000] loss_train: 0.1765 acc_train: 0.7520 acc_val: 0.7949\n",
      "Epoch: [555/1000] loss_train: 0.1698 acc_train: 0.7910 acc_val: 0.7949\n",
      "Epoch: [556/1000] loss_train: 0.1751 acc_train: 0.7623 acc_val: 0.7969\n",
      "Epoch: [557/1000] loss_train: 0.1797 acc_train: 0.7643 acc_val: 0.7988\n",
      "Epoch: [558/1000] loss_train: 0.1676 acc_train: 0.7684 acc_val: 0.7988\n",
      "Epoch: [559/1000] loss_train: 0.1704 acc_train: 0.7725 acc_val: 0.7988\n",
      "Epoch: [560/1000] loss_train: 0.1793 acc_train: 0.7623 acc_val: 0.7988\n",
      "Epoch: [561/1000] loss_train: 0.1679 acc_train: 0.7787 acc_val: 0.7988\n",
      "Epoch: [562/1000] loss_train: 0.1735 acc_train: 0.7664 acc_val: 0.7988\n",
      "Epoch: [563/1000] loss_train: 0.1646 acc_train: 0.7828 acc_val: 0.7988\n",
      "Epoch: [564/1000] loss_train: 0.1710 acc_train: 0.7664 acc_val: 0.8008\n",
      "Epoch: [565/1000] loss_train: 0.1680 acc_train: 0.7807 acc_val: 0.8008\n",
      "Epoch: [566/1000] loss_train: 0.1746 acc_train: 0.7746 acc_val: 0.7988\n",
      "Epoch: [567/1000] loss_train: 0.1790 acc_train: 0.7377 acc_val: 0.7988\n",
      "Epoch: [568/1000] loss_train: 0.1773 acc_train: 0.7520 acc_val: 0.7988\n",
      "Epoch: [569/1000] loss_train: 0.1742 acc_train: 0.7643 acc_val: 0.8008\n",
      "Epoch: [570/1000] loss_train: 0.1735 acc_train: 0.7787 acc_val: 0.8008\n",
      "Epoch: [571/1000] loss_train: 0.1748 acc_train: 0.7705 acc_val: 0.8008\n",
      "Epoch: [572/1000] loss_train: 0.1741 acc_train: 0.7643 acc_val: 0.8027\n",
      "Epoch: [573/1000] loss_train: 0.1754 acc_train: 0.7582 acc_val: 0.8047\n",
      "Epoch: [574/1000] loss_train: 0.1739 acc_train: 0.7582 acc_val: 0.8047\n",
      "Epoch: [575/1000] loss_train: 0.1695 acc_train: 0.7910 acc_val: 0.8047\n",
      "Epoch: [576/1000] loss_train: 0.1775 acc_train: 0.7582 acc_val: 0.8047\n",
      "Epoch: [577/1000] loss_train: 0.1755 acc_train: 0.7582 acc_val: 0.8047\n",
      "Epoch: [578/1000] loss_train: 0.1745 acc_train: 0.7602 acc_val: 0.8027\n",
      "Epoch: [579/1000] loss_train: 0.1701 acc_train: 0.7766 acc_val: 0.8027\n",
      "Epoch: [580/1000] loss_train: 0.1717 acc_train: 0.7643 acc_val: 0.8027\n",
      "Epoch: [581/1000] loss_train: 0.1746 acc_train: 0.7643 acc_val: 0.8027\n",
      "Epoch: [582/1000] loss_train: 0.1734 acc_train: 0.7807 acc_val: 0.8047\n",
      "Epoch: [583/1000] loss_train: 0.1736 acc_train: 0.7664 acc_val: 0.8047\n",
      "Epoch: [584/1000] loss_train: 0.1734 acc_train: 0.7664 acc_val: 0.8047\n",
      "Epoch: [585/1000] loss_train: 0.1785 acc_train: 0.7623 acc_val: 0.8086\n",
      "Epoch: [586/1000] loss_train: 0.1732 acc_train: 0.7623 acc_val: 0.8105\n",
      "Epoch: [587/1000] loss_train: 0.1737 acc_train: 0.7705 acc_val: 0.8105\n",
      "Epoch: [588/1000] loss_train: 0.1777 acc_train: 0.7439 acc_val: 0.8086\n",
      "Epoch: [589/1000] loss_train: 0.1664 acc_train: 0.7623 acc_val: 0.8086\n",
      "Epoch: [590/1000] loss_train: 0.1698 acc_train: 0.7746 acc_val: 0.8086\n",
      "Epoch: [591/1000] loss_train: 0.1777 acc_train: 0.7480 acc_val: 0.8066\n",
      "Epoch: [592/1000] loss_train: 0.1758 acc_train: 0.7602 acc_val: 0.8066\n",
      "Epoch: [593/1000] loss_train: 0.1649 acc_train: 0.7889 acc_val: 0.8086\n",
      "Epoch: [594/1000] loss_train: 0.1681 acc_train: 0.7807 acc_val: 0.8086\n",
      "Epoch: [595/1000] loss_train: 0.1664 acc_train: 0.7971 acc_val: 0.8086\n",
      "Epoch: [596/1000] loss_train: 0.1720 acc_train: 0.7561 acc_val: 0.8105\n",
      "Epoch: [597/1000] loss_train: 0.1735 acc_train: 0.7725 acc_val: 0.8125\n",
      "Epoch: [598/1000] loss_train: 0.1699 acc_train: 0.7869 acc_val: 0.8125\n",
      "Epoch: [599/1000] loss_train: 0.1656 acc_train: 0.7828 acc_val: 0.8145\n",
      "Epoch: [600/1000] loss_train: 0.1728 acc_train: 0.7746 acc_val: 0.8164\n",
      "Epoch: [601/1000] loss_train: 0.1632 acc_train: 0.7930 acc_val: 0.8164\n",
      "Epoch: [602/1000] loss_train: 0.1694 acc_train: 0.7848 acc_val: 0.8164\n",
      "Epoch: [603/1000] loss_train: 0.1749 acc_train: 0.7643 acc_val: 0.8164\n",
      "Epoch: [604/1000] loss_train: 0.1752 acc_train: 0.7520 acc_val: 0.8164\n",
      "Epoch: [605/1000] loss_train: 0.1721 acc_train: 0.7520 acc_val: 0.8184\n",
      "Epoch: [606/1000] loss_train: 0.1716 acc_train: 0.7541 acc_val: 0.8184\n",
      "Epoch: [607/1000] loss_train: 0.1656 acc_train: 0.7889 acc_val: 0.8184\n",
      "Epoch: [608/1000] loss_train: 0.1663 acc_train: 0.7828 acc_val: 0.8184\n",
      "Epoch: [609/1000] loss_train: 0.1720 acc_train: 0.7705 acc_val: 0.8203\n",
      "Epoch: [610/1000] loss_train: 0.1691 acc_train: 0.7930 acc_val: 0.8223\n",
      "Epoch: [611/1000] loss_train: 0.1708 acc_train: 0.7602 acc_val: 0.8223\n",
      "Epoch: [612/1000] loss_train: 0.1734 acc_train: 0.7480 acc_val: 0.8223\n",
      "Epoch: [613/1000] loss_train: 0.1677 acc_train: 0.7664 acc_val: 0.8223\n",
      "Epoch: [614/1000] loss_train: 0.1644 acc_train: 0.7848 acc_val: 0.8203\n",
      "Epoch: [615/1000] loss_train: 0.1629 acc_train: 0.7848 acc_val: 0.8184\n",
      "Epoch: [616/1000] loss_train: 0.1618 acc_train: 0.7889 acc_val: 0.8184\n",
      "Epoch: [617/1000] loss_train: 0.1643 acc_train: 0.7705 acc_val: 0.8203\n",
      "Epoch: [618/1000] loss_train: 0.1697 acc_train: 0.7623 acc_val: 0.8203\n",
      "Epoch: [619/1000] loss_train: 0.1708 acc_train: 0.7725 acc_val: 0.8203\n",
      "Epoch: [620/1000] loss_train: 0.1730 acc_train: 0.7828 acc_val: 0.8203\n",
      "Epoch: [621/1000] loss_train: 0.1715 acc_train: 0.7705 acc_val: 0.8203\n",
      "Epoch: [622/1000] loss_train: 0.1751 acc_train: 0.7684 acc_val: 0.8223\n",
      "Epoch: [623/1000] loss_train: 0.1711 acc_train: 0.7602 acc_val: 0.8223\n",
      "Epoch: [624/1000] loss_train: 0.1709 acc_train: 0.7746 acc_val: 0.8223\n",
      "Epoch: [625/1000] loss_train: 0.1735 acc_train: 0.7889 acc_val: 0.8223\n",
      "Epoch: [626/1000] loss_train: 0.1710 acc_train: 0.7705 acc_val: 0.8223\n",
      "Epoch: [627/1000] loss_train: 0.1622 acc_train: 0.7828 acc_val: 0.8223\n",
      "Epoch: [628/1000] loss_train: 0.1615 acc_train: 0.7930 acc_val: 0.8223\n",
      "Epoch: [629/1000] loss_train: 0.1639 acc_train: 0.7746 acc_val: 0.8223\n",
      "Epoch: [630/1000] loss_train: 0.1653 acc_train: 0.7910 acc_val: 0.8223\n",
      "Epoch: [631/1000] loss_train: 0.1712 acc_train: 0.7602 acc_val: 0.8262\n",
      "Epoch: [632/1000] loss_train: 0.1639 acc_train: 0.7930 acc_val: 0.8262\n",
      "Epoch: [633/1000] loss_train: 0.1663 acc_train: 0.7848 acc_val: 0.8242\n",
      "Epoch: [634/1000] loss_train: 0.1633 acc_train: 0.7889 acc_val: 0.8223\n",
      "Epoch: [635/1000] loss_train: 0.1688 acc_train: 0.7951 acc_val: 0.8223\n",
      "Epoch: [636/1000] loss_train: 0.1637 acc_train: 0.8053 acc_val: 0.8242\n",
      "Epoch: [637/1000] loss_train: 0.1619 acc_train: 0.7807 acc_val: 0.8242\n",
      "Epoch: [638/1000] loss_train: 0.1706 acc_train: 0.7684 acc_val: 0.8242\n",
      "Epoch: [639/1000] loss_train: 0.1717 acc_train: 0.7787 acc_val: 0.8242\n",
      "Epoch: [640/1000] loss_train: 0.1615 acc_train: 0.7910 acc_val: 0.8242\n",
      "Epoch: [641/1000] loss_train: 0.1649 acc_train: 0.7971 acc_val: 0.8242\n",
      "Epoch: [642/1000] loss_train: 0.1635 acc_train: 0.7828 acc_val: 0.8242\n",
      "Epoch: [643/1000] loss_train: 0.1623 acc_train: 0.7787 acc_val: 0.8242\n",
      "Epoch: [644/1000] loss_train: 0.1581 acc_train: 0.8012 acc_val: 0.8242\n",
      "Epoch: [645/1000] loss_train: 0.1641 acc_train: 0.7705 acc_val: 0.8242\n",
      "Epoch: [646/1000] loss_train: 0.1704 acc_train: 0.7869 acc_val: 0.8242\n",
      "Epoch: [647/1000] loss_train: 0.1679 acc_train: 0.7889 acc_val: 0.8281\n",
      "Epoch: [648/1000] loss_train: 0.1655 acc_train: 0.7746 acc_val: 0.8281\n",
      "Epoch: [649/1000] loss_train: 0.1626 acc_train: 0.7951 acc_val: 0.8281\n",
      "Epoch: [650/1000] loss_train: 0.1662 acc_train: 0.7725 acc_val: 0.8262\n",
      "Epoch: [651/1000] loss_train: 0.1635 acc_train: 0.7828 acc_val: 0.8262\n",
      "Epoch: [652/1000] loss_train: 0.1614 acc_train: 0.7930 acc_val: 0.8262\n",
      "Epoch: [653/1000] loss_train: 0.1755 acc_train: 0.7684 acc_val: 0.8262\n",
      "Epoch: [654/1000] loss_train: 0.1702 acc_train: 0.7725 acc_val: 0.8262\n",
      "Epoch: [655/1000] loss_train: 0.1647 acc_train: 0.7930 acc_val: 0.8262\n",
      "Epoch: [656/1000] loss_train: 0.1705 acc_train: 0.7705 acc_val: 0.8262\n",
      "Epoch: [657/1000] loss_train: 0.1617 acc_train: 0.7951 acc_val: 0.8262\n",
      "Epoch: [658/1000] loss_train: 0.1654 acc_train: 0.7766 acc_val: 0.8262\n",
      "Epoch: [659/1000] loss_train: 0.1677 acc_train: 0.7889 acc_val: 0.8262\n",
      "Epoch: [660/1000] loss_train: 0.1642 acc_train: 0.7848 acc_val: 0.8281\n",
      "Epoch: [661/1000] loss_train: 0.1625 acc_train: 0.7869 acc_val: 0.8281\n",
      "Epoch: [662/1000] loss_train: 0.1626 acc_train: 0.7828 acc_val: 0.8281\n",
      "Epoch: [663/1000] loss_train: 0.1719 acc_train: 0.7623 acc_val: 0.8281\n",
      "Epoch: [664/1000] loss_train: 0.1695 acc_train: 0.7746 acc_val: 0.8281\n",
      "Epoch: [665/1000] loss_train: 0.1578 acc_train: 0.7848 acc_val: 0.8301\n",
      "Epoch: [666/1000] loss_train: 0.1651 acc_train: 0.7725 acc_val: 0.8301\n",
      "Epoch: [667/1000] loss_train: 0.1586 acc_train: 0.7930 acc_val: 0.8301\n",
      "Epoch: [668/1000] loss_train: 0.1671 acc_train: 0.7910 acc_val: 0.8301\n",
      "Epoch: [669/1000] loss_train: 0.1619 acc_train: 0.7869 acc_val: 0.8320\n",
      "Epoch: [670/1000] loss_train: 0.1668 acc_train: 0.7828 acc_val: 0.8320\n",
      "Epoch: [671/1000] loss_train: 0.1746 acc_train: 0.7623 acc_val: 0.8320\n",
      "Epoch: [672/1000] loss_train: 0.1560 acc_train: 0.8156 acc_val: 0.8340\n",
      "Epoch: [673/1000] loss_train: 0.1694 acc_train: 0.7951 acc_val: 0.8340\n",
      "Epoch: [674/1000] loss_train: 0.1608 acc_train: 0.7869 acc_val: 0.8340\n",
      "Epoch: [675/1000] loss_train: 0.1668 acc_train: 0.7746 acc_val: 0.8340\n",
      "Epoch: [676/1000] loss_train: 0.1640 acc_train: 0.7889 acc_val: 0.8359\n",
      "Epoch: [677/1000] loss_train: 0.1637 acc_train: 0.7869 acc_val: 0.8359\n",
      "Epoch: [678/1000] loss_train: 0.1566 acc_train: 0.8094 acc_val: 0.8359\n",
      "Epoch: [679/1000] loss_train: 0.1657 acc_train: 0.7869 acc_val: 0.8359\n",
      "Epoch: [680/1000] loss_train: 0.1629 acc_train: 0.7910 acc_val: 0.8359\n",
      "Epoch: [681/1000] loss_train: 0.1610 acc_train: 0.8012 acc_val: 0.8359\n",
      "Epoch: [682/1000] loss_train: 0.1643 acc_train: 0.7910 acc_val: 0.8359\n",
      "Epoch: [683/1000] loss_train: 0.1674 acc_train: 0.7643 acc_val: 0.8359\n",
      "Epoch: [684/1000] loss_train: 0.1668 acc_train: 0.7746 acc_val: 0.8398\n",
      "Acc_best is updated to 0.8398. Model checkpoint is saved to ./model/BA1k_MVC_Student.pt\n",
      "Test accuracy is 0.8250\n",
      "Epoch: [685/1000] loss_train: 0.1629 acc_train: 0.7930 acc_val: 0.8398\n",
      "Epoch: [686/1000] loss_train: 0.1620 acc_train: 0.7930 acc_val: 0.8398\n",
      "Epoch: [687/1000] loss_train: 0.1652 acc_train: 0.7705 acc_val: 0.8398\n",
      "Epoch: [688/1000] loss_train: 0.1727 acc_train: 0.7480 acc_val: 0.8398\n",
      "Epoch: [689/1000] loss_train: 0.1652 acc_train: 0.7746 acc_val: 0.8398\n",
      "Epoch: [690/1000] loss_train: 0.1589 acc_train: 0.7848 acc_val: 0.8398\n",
      "Epoch: [691/1000] loss_train: 0.1607 acc_train: 0.7787 acc_val: 0.8398\n",
      "Epoch: [692/1000] loss_train: 0.1671 acc_train: 0.7910 acc_val: 0.8398\n",
      "Epoch: [693/1000] loss_train: 0.1640 acc_train: 0.7889 acc_val: 0.8398\n",
      "Epoch: [694/1000] loss_train: 0.1635 acc_train: 0.7705 acc_val: 0.8398\n",
      "Epoch: [695/1000] loss_train: 0.1615 acc_train: 0.7869 acc_val: 0.8398\n",
      "Epoch: [696/1000] loss_train: 0.1641 acc_train: 0.7705 acc_val: 0.8398\n",
      "Epoch: [697/1000] loss_train: 0.1603 acc_train: 0.7828 acc_val: 0.8398\n",
      "Epoch: [698/1000] loss_train: 0.1685 acc_train: 0.7766 acc_val: 0.8398\n",
      "Epoch: [699/1000] loss_train: 0.1652 acc_train: 0.7828 acc_val: 0.8379\n",
      "Epoch: [700/1000] loss_train: 0.1688 acc_train: 0.7643 acc_val: 0.8340\n",
      "Epoch: [701/1000] loss_train: 0.1690 acc_train: 0.7725 acc_val: 0.8340\n",
      "Epoch: [702/1000] loss_train: 0.1625 acc_train: 0.7746 acc_val: 0.8340\n",
      "Epoch: [703/1000] loss_train: 0.1676 acc_train: 0.7766 acc_val: 0.8340\n",
      "Epoch: [704/1000] loss_train: 0.1655 acc_train: 0.7766 acc_val: 0.8340\n",
      "Epoch: [705/1000] loss_train: 0.1652 acc_train: 0.7807 acc_val: 0.8340\n",
      "Epoch: [706/1000] loss_train: 0.1675 acc_train: 0.7828 acc_val: 0.8340\n",
      "Epoch: [707/1000] loss_train: 0.1685 acc_train: 0.7705 acc_val: 0.8340\n",
      "Epoch: [708/1000] loss_train: 0.1724 acc_train: 0.7684 acc_val: 0.8340\n",
      "Epoch: [709/1000] loss_train: 0.1668 acc_train: 0.7889 acc_val: 0.8340\n",
      "Epoch: [710/1000] loss_train: 0.1615 acc_train: 0.7992 acc_val: 0.8340\n",
      "Epoch: [711/1000] loss_train: 0.1654 acc_train: 0.7746 acc_val: 0.8340\n",
      "Epoch: [712/1000] loss_train: 0.1582 acc_train: 0.7889 acc_val: 0.8340\n",
      "Epoch: [713/1000] loss_train: 0.1676 acc_train: 0.7951 acc_val: 0.8340\n",
      "Epoch: [714/1000] loss_train: 0.1605 acc_train: 0.7746 acc_val: 0.8340\n",
      "Epoch: [715/1000] loss_train: 0.1544 acc_train: 0.8012 acc_val: 0.8340\n",
      "Epoch: [716/1000] loss_train: 0.1624 acc_train: 0.7910 acc_val: 0.8340\n",
      "Epoch: [717/1000] loss_train: 0.1644 acc_train: 0.7705 acc_val: 0.8340\n",
      "Epoch: [718/1000] loss_train: 0.1662 acc_train: 0.7766 acc_val: 0.8340\n",
      "Epoch: [719/1000] loss_train: 0.1650 acc_train: 0.7684 acc_val: 0.8340\n",
      "Epoch: [720/1000] loss_train: 0.1649 acc_train: 0.7910 acc_val: 0.8320\n",
      "Epoch: [721/1000] loss_train: 0.1680 acc_train: 0.7664 acc_val: 0.8320\n",
      "Epoch: [722/1000] loss_train: 0.1666 acc_train: 0.7602 acc_val: 0.8320\n",
      "Epoch: [723/1000] loss_train: 0.1635 acc_train: 0.7766 acc_val: 0.8320\n",
      "Epoch: [724/1000] loss_train: 0.1643 acc_train: 0.7807 acc_val: 0.8320\n",
      "Epoch: [725/1000] loss_train: 0.1647 acc_train: 0.7828 acc_val: 0.8320\n",
      "Epoch: [726/1000] loss_train: 0.1672 acc_train: 0.7746 acc_val: 0.8320\n",
      "Epoch: [727/1000] loss_train: 0.1581 acc_train: 0.8115 acc_val: 0.8320\n",
      "Epoch: [728/1000] loss_train: 0.1625 acc_train: 0.7807 acc_val: 0.8320\n",
      "Epoch: [729/1000] loss_train: 0.1647 acc_train: 0.7623 acc_val: 0.8320\n",
      "Epoch: [730/1000] loss_train: 0.1611 acc_train: 0.7889 acc_val: 0.8320\n",
      "Epoch: [731/1000] loss_train: 0.1659 acc_train: 0.7807 acc_val: 0.8320\n",
      "Epoch: [732/1000] loss_train: 0.1589 acc_train: 0.8033 acc_val: 0.8320\n",
      "Epoch: [733/1000] loss_train: 0.1625 acc_train: 0.7705 acc_val: 0.8320\n",
      "Epoch: [734/1000] loss_train: 0.1634 acc_train: 0.7807 acc_val: 0.8320\n",
      "Epoch: [735/1000] loss_train: 0.1604 acc_train: 0.7910 acc_val: 0.8320\n",
      "Epoch: [736/1000] loss_train: 0.1634 acc_train: 0.7889 acc_val: 0.8320\n",
      "Epoch: [737/1000] loss_train: 0.1595 acc_train: 0.7951 acc_val: 0.8320\n",
      "Epoch: [738/1000] loss_train: 0.1580 acc_train: 0.7992 acc_val: 0.8320\n",
      "Epoch: [739/1000] loss_train: 0.1616 acc_train: 0.8012 acc_val: 0.8320\n",
      "Epoch: [740/1000] loss_train: 0.1608 acc_train: 0.7992 acc_val: 0.8320\n",
      "Epoch: [741/1000] loss_train: 0.1605 acc_train: 0.8033 acc_val: 0.8320\n",
      "Epoch: [742/1000] loss_train: 0.1703 acc_train: 0.7643 acc_val: 0.8320\n",
      "Epoch: [743/1000] loss_train: 0.1617 acc_train: 0.7766 acc_val: 0.8320\n",
      "Epoch: [744/1000] loss_train: 0.1592 acc_train: 0.7889 acc_val: 0.8340\n",
      "Epoch: [745/1000] loss_train: 0.1588 acc_train: 0.8074 acc_val: 0.8340\n",
      "Epoch: [746/1000] loss_train: 0.1703 acc_train: 0.7807 acc_val: 0.8340\n",
      "Epoch: [747/1000] loss_train: 0.1654 acc_train: 0.7828 acc_val: 0.8359\n",
      "Epoch: [748/1000] loss_train: 0.1566 acc_train: 0.8115 acc_val: 0.8340\n",
      "Epoch: [749/1000] loss_train: 0.1622 acc_train: 0.7910 acc_val: 0.8340\n",
      "Epoch: [750/1000] loss_train: 0.1652 acc_train: 0.7807 acc_val: 0.8340\n",
      "Epoch: [751/1000] loss_train: 0.1638 acc_train: 0.7725 acc_val: 0.8340\n",
      "Epoch: [752/1000] loss_train: 0.1641 acc_train: 0.7602 acc_val: 0.8340\n",
      "Epoch: [753/1000] loss_train: 0.1552 acc_train: 0.8115 acc_val: 0.8340\n",
      "Epoch: [754/1000] loss_train: 0.1630 acc_train: 0.7889 acc_val: 0.8340\n",
      "Epoch: [755/1000] loss_train: 0.1654 acc_train: 0.7787 acc_val: 0.8340\n",
      "Epoch: [756/1000] loss_train: 0.1616 acc_train: 0.7889 acc_val: 0.8340\n",
      "Epoch: [757/1000] loss_train: 0.1666 acc_train: 0.7664 acc_val: 0.8340\n",
      "Epoch: [758/1000] loss_train: 0.1590 acc_train: 0.7848 acc_val: 0.8340\n",
      "Epoch: [759/1000] loss_train: 0.1624 acc_train: 0.7807 acc_val: 0.8340\n",
      "Epoch: [760/1000] loss_train: 0.1659 acc_train: 0.7869 acc_val: 0.8340\n",
      "Epoch: [761/1000] loss_train: 0.1679 acc_train: 0.7602 acc_val: 0.8340\n",
      "Epoch: [762/1000] loss_train: 0.1592 acc_train: 0.7951 acc_val: 0.8340\n",
      "Epoch: [763/1000] loss_train: 0.1628 acc_train: 0.7807 acc_val: 0.8340\n",
      "Epoch: [764/1000] loss_train: 0.1628 acc_train: 0.7848 acc_val: 0.8340\n",
      "Epoch: [765/1000] loss_train: 0.1650 acc_train: 0.7705 acc_val: 0.8340\n",
      "Epoch: [766/1000] loss_train: 0.1589 acc_train: 0.7807 acc_val: 0.8340\n",
      "Epoch: [767/1000] loss_train: 0.1633 acc_train: 0.7746 acc_val: 0.8340\n",
      "Epoch: [768/1000] loss_train: 0.1671 acc_train: 0.7602 acc_val: 0.8340\n",
      "Epoch: [769/1000] loss_train: 0.1594 acc_train: 0.8033 acc_val: 0.8340\n",
      "Epoch: [770/1000] loss_train: 0.1585 acc_train: 0.7992 acc_val: 0.8340\n",
      "Epoch: [771/1000] loss_train: 0.1603 acc_train: 0.7971 acc_val: 0.8340\n",
      "Epoch: [772/1000] loss_train: 0.1625 acc_train: 0.7951 acc_val: 0.8340\n",
      "Epoch: [773/1000] loss_train: 0.1612 acc_train: 0.7787 acc_val: 0.8340\n",
      "Epoch: [774/1000] loss_train: 0.1614 acc_train: 0.7725 acc_val: 0.8340\n",
      "Epoch: [775/1000] loss_train: 0.1638 acc_train: 0.8135 acc_val: 0.8340\n",
      "Epoch: [776/1000] loss_train: 0.1638 acc_train: 0.7971 acc_val: 0.8340\n",
      "Epoch: [777/1000] loss_train: 0.1604 acc_train: 0.7930 acc_val: 0.8340\n",
      "Epoch: [778/1000] loss_train: 0.1588 acc_train: 0.8012 acc_val: 0.8340\n",
      "Epoch: [779/1000] loss_train: 0.1580 acc_train: 0.7725 acc_val: 0.8340\n",
      "Epoch: [780/1000] loss_train: 0.1638 acc_train: 0.7828 acc_val: 0.8340\n",
      "Epoch: [781/1000] loss_train: 0.1645 acc_train: 0.7807 acc_val: 0.8340\n",
      "Epoch: [782/1000] loss_train: 0.1633 acc_train: 0.7787 acc_val: 0.8320\n",
      "Epoch: [783/1000] loss_train: 0.1628 acc_train: 0.7766 acc_val: 0.8320\n",
      "Epoch: [784/1000] loss_train: 0.1577 acc_train: 0.7869 acc_val: 0.8320\n",
      "Epoch: [785/1000] loss_train: 0.1639 acc_train: 0.7807 acc_val: 0.8320\n",
      "Epoch: [786/1000] loss_train: 0.1605 acc_train: 0.7889 acc_val: 0.8320\n",
      "Epoch: [787/1000] loss_train: 0.1617 acc_train: 0.8053 acc_val: 0.8320\n",
      "Epoch: [788/1000] loss_train: 0.1606 acc_train: 0.7910 acc_val: 0.8340\n",
      "Epoch: [789/1000] loss_train: 0.1620 acc_train: 0.7807 acc_val: 0.8340\n",
      "Epoch: [790/1000] loss_train: 0.1605 acc_train: 0.7869 acc_val: 0.8340\n",
      "Epoch: [791/1000] loss_train: 0.1618 acc_train: 0.7951 acc_val: 0.8340\n",
      "Epoch: [792/1000] loss_train: 0.1601 acc_train: 0.8053 acc_val: 0.8340\n",
      "Epoch: [793/1000] loss_train: 0.1601 acc_train: 0.7930 acc_val: 0.8340\n",
      "Epoch: [794/1000] loss_train: 0.1626 acc_train: 0.7705 acc_val: 0.8340\n",
      "Epoch: [795/1000] loss_train: 0.1605 acc_train: 0.7910 acc_val: 0.8340\n",
      "Epoch: [796/1000] loss_train: 0.1626 acc_train: 0.7848 acc_val: 0.8320\n",
      "Epoch: [797/1000] loss_train: 0.1639 acc_train: 0.7889 acc_val: 0.8320\n",
      "Epoch: [798/1000] loss_train: 0.1657 acc_train: 0.7623 acc_val: 0.8320\n",
      "Epoch: [799/1000] loss_train: 0.1535 acc_train: 0.7992 acc_val: 0.8340\n",
      "Epoch: [800/1000] loss_train: 0.1586 acc_train: 0.7787 acc_val: 0.8340\n",
      "Epoch: [801/1000] loss_train: 0.1647 acc_train: 0.7746 acc_val: 0.8340\n",
      "Epoch: [802/1000] loss_train: 0.1619 acc_train: 0.7910 acc_val: 0.8340\n",
      "Epoch: [803/1000] loss_train: 0.1624 acc_train: 0.7889 acc_val: 0.8340\n",
      "Epoch: [804/1000] loss_train: 0.1659 acc_train: 0.7643 acc_val: 0.8320\n",
      "Epoch: [805/1000] loss_train: 0.1673 acc_train: 0.7725 acc_val: 0.8320\n",
      "Epoch: [806/1000] loss_train: 0.1620 acc_train: 0.7746 acc_val: 0.8320\n",
      "Epoch: [807/1000] loss_train: 0.1599 acc_train: 0.7725 acc_val: 0.8320\n",
      "Epoch: [808/1000] loss_train: 0.1661 acc_train: 0.7664 acc_val: 0.8320\n",
      "Epoch: [809/1000] loss_train: 0.1561 acc_train: 0.7992 acc_val: 0.8320\n",
      "Epoch: [810/1000] loss_train: 0.1637 acc_train: 0.7684 acc_val: 0.8320\n",
      "Epoch: [811/1000] loss_train: 0.1612 acc_train: 0.7828 acc_val: 0.8320\n",
      "Epoch: [812/1000] loss_train: 0.1601 acc_train: 0.7930 acc_val: 0.8340\n",
      "Epoch: [813/1000] loss_train: 0.1617 acc_train: 0.7684 acc_val: 0.8320\n",
      "Epoch: [814/1000] loss_train: 0.1594 acc_train: 0.7992 acc_val: 0.8320\n",
      "Epoch: [815/1000] loss_train: 0.1578 acc_train: 0.8074 acc_val: 0.8320\n",
      "Epoch: [816/1000] loss_train: 0.1645 acc_train: 0.7807 acc_val: 0.8320\n",
      "Epoch: [817/1000] loss_train: 0.1591 acc_train: 0.8012 acc_val: 0.8320\n",
      "Epoch: [818/1000] loss_train: 0.1614 acc_train: 0.7869 acc_val: 0.8320\n",
      "Epoch: [819/1000] loss_train: 0.1601 acc_train: 0.7807 acc_val: 0.8281\n",
      "Epoch: [820/1000] loss_train: 0.1579 acc_train: 0.8053 acc_val: 0.8281\n",
      "Epoch: [821/1000] loss_train: 0.1620 acc_train: 0.7766 acc_val: 0.8281\n",
      "Epoch: [822/1000] loss_train: 0.1603 acc_train: 0.7930 acc_val: 0.8281\n",
      "Epoch: [823/1000] loss_train: 0.1609 acc_train: 0.8012 acc_val: 0.8281\n",
      "Epoch: [824/1000] loss_train: 0.1648 acc_train: 0.7705 acc_val: 0.8281\n",
      "Epoch: [825/1000] loss_train: 0.1584 acc_train: 0.7992 acc_val: 0.8262\n",
      "Epoch: [826/1000] loss_train: 0.1604 acc_train: 0.8012 acc_val: 0.8262\n",
      "Epoch: [827/1000] loss_train: 0.1600 acc_train: 0.7910 acc_val: 0.8262\n",
      "Epoch: [828/1000] loss_train: 0.1607 acc_train: 0.7869 acc_val: 0.8262\n",
      "Epoch: [829/1000] loss_train: 0.1566 acc_train: 0.8115 acc_val: 0.8262\n",
      "Epoch: [830/1000] loss_train: 0.1656 acc_train: 0.7582 acc_val: 0.8262\n",
      "Epoch: [831/1000] loss_train: 0.1612 acc_train: 0.8074 acc_val: 0.8262\n",
      "Epoch: [832/1000] loss_train: 0.1561 acc_train: 0.8115 acc_val: 0.8262\n",
      "Epoch: [833/1000] loss_train: 0.1518 acc_train: 0.8197 acc_val: 0.8262\n",
      "Epoch: [834/1000] loss_train: 0.1675 acc_train: 0.7602 acc_val: 0.8262\n",
      "Epoch: [835/1000] loss_train: 0.1562 acc_train: 0.7930 acc_val: 0.8262\n",
      "Epoch: [836/1000] loss_train: 0.1641 acc_train: 0.7807 acc_val: 0.8262\n",
      "Epoch: [837/1000] loss_train: 0.1593 acc_train: 0.7889 acc_val: 0.8262\n",
      "Epoch: [838/1000] loss_train: 0.1600 acc_train: 0.8238 acc_val: 0.8262\n",
      "Epoch: [839/1000] loss_train: 0.1558 acc_train: 0.8053 acc_val: 0.8262\n",
      "Epoch: [840/1000] loss_train: 0.1577 acc_train: 0.7971 acc_val: 0.8262\n",
      "Epoch: [841/1000] loss_train: 0.1566 acc_train: 0.7889 acc_val: 0.8281\n",
      "Epoch: [842/1000] loss_train: 0.1592 acc_train: 0.8012 acc_val: 0.8281\n",
      "Epoch: [843/1000] loss_train: 0.1587 acc_train: 0.8074 acc_val: 0.8262\n",
      "Epoch: [844/1000] loss_train: 0.1574 acc_train: 0.7910 acc_val: 0.8242\n",
      "Epoch: [845/1000] loss_train: 0.1561 acc_train: 0.7910 acc_val: 0.8242\n",
      "Epoch: [846/1000] loss_train: 0.1610 acc_train: 0.7848 acc_val: 0.8242\n",
      "Epoch: [847/1000] loss_train: 0.1655 acc_train: 0.7746 acc_val: 0.8242\n",
      "Epoch: [848/1000] loss_train: 0.1555 acc_train: 0.8074 acc_val: 0.8242\n",
      "Epoch: [849/1000] loss_train: 0.1588 acc_train: 0.8156 acc_val: 0.8242\n",
      "Epoch: [850/1000] loss_train: 0.1596 acc_train: 0.7930 acc_val: 0.8242\n",
      "Epoch: [851/1000] loss_train: 0.1588 acc_train: 0.7807 acc_val: 0.8242\n",
      "Epoch: [852/1000] loss_train: 0.1605 acc_train: 0.7930 acc_val: 0.8262\n",
      "Epoch: [853/1000] loss_train: 0.1561 acc_train: 0.8033 acc_val: 0.8262\n",
      "Epoch: [854/1000] loss_train: 0.1569 acc_train: 0.8115 acc_val: 0.8242\n",
      "Epoch: [855/1000] loss_train: 0.1576 acc_train: 0.8033 acc_val: 0.8262\n",
      "Epoch: [856/1000] loss_train: 0.1600 acc_train: 0.7869 acc_val: 0.8262\n",
      "Epoch: [857/1000] loss_train: 0.1625 acc_train: 0.7910 acc_val: 0.8262\n",
      "Epoch: [858/1000] loss_train: 0.1602 acc_train: 0.7807 acc_val: 0.8262\n",
      "Epoch: [859/1000] loss_train: 0.1620 acc_train: 0.7746 acc_val: 0.8262\n",
      "Epoch: [860/1000] loss_train: 0.1619 acc_train: 0.7725 acc_val: 0.8262\n",
      "Epoch: [861/1000] loss_train: 0.1581 acc_train: 0.7869 acc_val: 0.8262\n",
      "Epoch: [862/1000] loss_train: 0.1611 acc_train: 0.7623 acc_val: 0.8262\n",
      "Epoch: [863/1000] loss_train: 0.1610 acc_train: 0.7889 acc_val: 0.8262\n",
      "Epoch: [864/1000] loss_train: 0.1560 acc_train: 0.7828 acc_val: 0.8262\n",
      "Epoch: [865/1000] loss_train: 0.1593 acc_train: 0.7889 acc_val: 0.8262\n",
      "Epoch: [866/1000] loss_train: 0.1593 acc_train: 0.7971 acc_val: 0.8262\n",
      "Epoch: [867/1000] loss_train: 0.1568 acc_train: 0.8074 acc_val: 0.8262\n",
      "Epoch: [868/1000] loss_train: 0.1601 acc_train: 0.8033 acc_val: 0.8262\n",
      "Epoch: [869/1000] loss_train: 0.1653 acc_train: 0.7828 acc_val: 0.8281\n",
      "Epoch: [870/1000] loss_train: 0.1647 acc_train: 0.7623 acc_val: 0.8262\n",
      "Epoch: [871/1000] loss_train: 0.1567 acc_train: 0.7848 acc_val: 0.8242\n",
      "Epoch: [872/1000] loss_train: 0.1521 acc_train: 0.8053 acc_val: 0.8242\n",
      "Epoch: [873/1000] loss_train: 0.1582 acc_train: 0.7910 acc_val: 0.8223\n",
      "Epoch: [874/1000] loss_train: 0.1577 acc_train: 0.7971 acc_val: 0.8223\n",
      "Epoch: [875/1000] loss_train: 0.1582 acc_train: 0.7705 acc_val: 0.8223\n",
      "Epoch: [876/1000] loss_train: 0.1594 acc_train: 0.7848 acc_val: 0.8223\n",
      "Epoch: [877/1000] loss_train: 0.1607 acc_train: 0.8094 acc_val: 0.8223\n",
      "Epoch: [878/1000] loss_train: 0.1560 acc_train: 0.7992 acc_val: 0.8223\n",
      "Epoch: [879/1000] loss_train: 0.1614 acc_train: 0.7910 acc_val: 0.8223\n",
      "Epoch: [880/1000] loss_train: 0.1569 acc_train: 0.8115 acc_val: 0.8203\n",
      "Epoch: [881/1000] loss_train: 0.1543 acc_train: 0.7930 acc_val: 0.8203\n",
      "Epoch: [882/1000] loss_train: 0.1523 acc_train: 0.8033 acc_val: 0.8203\n",
      "Epoch: [883/1000] loss_train: 0.1562 acc_train: 0.8033 acc_val: 0.8203\n",
      "Epoch: [884/1000] loss_train: 0.1568 acc_train: 0.7992 acc_val: 0.8203\n",
      "Epoch: [885/1000] loss_train: 0.1555 acc_train: 0.7951 acc_val: 0.8203\n",
      "Epoch: [886/1000] loss_train: 0.1622 acc_train: 0.7787 acc_val: 0.8203\n",
      "Epoch: [887/1000] loss_train: 0.1582 acc_train: 0.7951 acc_val: 0.8184\n",
      "Epoch: [888/1000] loss_train: 0.1606 acc_train: 0.7746 acc_val: 0.8164\n",
      "Epoch: [889/1000] loss_train: 0.1581 acc_train: 0.8033 acc_val: 0.8164\n",
      "Epoch: [890/1000] loss_train: 0.1564 acc_train: 0.8115 acc_val: 0.8164\n",
      "Epoch: [891/1000] loss_train: 0.1557 acc_train: 0.8074 acc_val: 0.8164\n",
      "Epoch: [892/1000] loss_train: 0.1498 acc_train: 0.8156 acc_val: 0.8145\n",
      "Epoch: [893/1000] loss_train: 0.1560 acc_train: 0.8094 acc_val: 0.8125\n",
      "Epoch: [894/1000] loss_train: 0.1591 acc_train: 0.7951 acc_val: 0.8105\n",
      "Epoch: [895/1000] loss_train: 0.1631 acc_train: 0.7705 acc_val: 0.8105\n",
      "Epoch: [896/1000] loss_train: 0.1567 acc_train: 0.8053 acc_val: 0.8105\n",
      "Epoch: [897/1000] loss_train: 0.1559 acc_train: 0.8053 acc_val: 0.8105\n",
      "Epoch: [898/1000] loss_train: 0.1621 acc_train: 0.7869 acc_val: 0.8105\n",
      "Epoch: [899/1000] loss_train: 0.1676 acc_train: 0.7930 acc_val: 0.8105\n",
      "Epoch: [900/1000] loss_train: 0.1581 acc_train: 0.7643 acc_val: 0.8086\n",
      "Epoch: [901/1000] loss_train: 0.1605 acc_train: 0.7930 acc_val: 0.8086\n",
      "Epoch: [902/1000] loss_train: 0.1656 acc_train: 0.7459 acc_val: 0.8086\n",
      "Epoch: [903/1000] loss_train: 0.1593 acc_train: 0.7869 acc_val: 0.8086\n",
      "Epoch: [904/1000] loss_train: 0.1589 acc_train: 0.8012 acc_val: 0.8086\n",
      "Epoch: [905/1000] loss_train: 0.1674 acc_train: 0.7910 acc_val: 0.8105\n",
      "Epoch: [906/1000] loss_train: 0.1573 acc_train: 0.7787 acc_val: 0.8105\n",
      "Epoch: [907/1000] loss_train: 0.1519 acc_train: 0.8094 acc_val: 0.8105\n",
      "Epoch: [908/1000] loss_train: 0.1595 acc_train: 0.7910 acc_val: 0.8105\n",
      "Epoch: [909/1000] loss_train: 0.1573 acc_train: 0.7992 acc_val: 0.8105\n",
      "Epoch: [910/1000] loss_train: 0.1586 acc_train: 0.7951 acc_val: 0.8105\n",
      "Epoch: [911/1000] loss_train: 0.1611 acc_train: 0.7910 acc_val: 0.8105\n",
      "Epoch: [912/1000] loss_train: 0.1554 acc_train: 0.8074 acc_val: 0.8105\n",
      "Epoch: [913/1000] loss_train: 0.1593 acc_train: 0.7992 acc_val: 0.8105\n",
      "Epoch: [914/1000] loss_train: 0.1596 acc_train: 0.8033 acc_val: 0.8105\n",
      "Epoch: [915/1000] loss_train: 0.1639 acc_train: 0.7787 acc_val: 0.8086\n",
      "Epoch: [916/1000] loss_train: 0.1556 acc_train: 0.7848 acc_val: 0.8086\n",
      "Epoch: [917/1000] loss_train: 0.1549 acc_train: 0.7992 acc_val: 0.8086\n",
      "Epoch: [918/1000] loss_train: 0.1594 acc_train: 0.7807 acc_val: 0.8086\n",
      "Epoch: [919/1000] loss_train: 0.1571 acc_train: 0.8135 acc_val: 0.8086\n",
      "Epoch: [920/1000] loss_train: 0.1599 acc_train: 0.7787 acc_val: 0.8105\n",
      "Epoch: [921/1000] loss_train: 0.1554 acc_train: 0.7930 acc_val: 0.8105\n",
      "Epoch: [922/1000] loss_train: 0.1562 acc_train: 0.8012 acc_val: 0.8105\n",
      "Epoch: [923/1000] loss_train: 0.1533 acc_train: 0.8033 acc_val: 0.8086\n",
      "Epoch: [924/1000] loss_train: 0.1589 acc_train: 0.7746 acc_val: 0.8086\n",
      "Epoch: [925/1000] loss_train: 0.1544 acc_train: 0.8094 acc_val: 0.8086\n",
      "Epoch: [926/1000] loss_train: 0.1604 acc_train: 0.7910 acc_val: 0.8086\n",
      "Epoch: [927/1000] loss_train: 0.1586 acc_train: 0.7869 acc_val: 0.8086\n",
      "Epoch: [928/1000] loss_train: 0.1557 acc_train: 0.8094 acc_val: 0.8086\n",
      "Epoch: [929/1000] loss_train: 0.1588 acc_train: 0.7828 acc_val: 0.8105\n",
      "Epoch: [930/1000] loss_train: 0.1525 acc_train: 0.7992 acc_val: 0.8105\n",
      "Epoch: [931/1000] loss_train: 0.1516 acc_train: 0.8156 acc_val: 0.8105\n",
      "Epoch: [932/1000] loss_train: 0.1644 acc_train: 0.7623 acc_val: 0.8105\n",
      "Epoch: [933/1000] loss_train: 0.1567 acc_train: 0.7951 acc_val: 0.8105\n",
      "Epoch: [934/1000] loss_train: 0.1567 acc_train: 0.7971 acc_val: 0.8105\n",
      "Epoch: [935/1000] loss_train: 0.1599 acc_train: 0.7971 acc_val: 0.8105\n",
      "Epoch: [936/1000] loss_train: 0.1555 acc_train: 0.8094 acc_val: 0.8105\n",
      "Epoch: [937/1000] loss_train: 0.1615 acc_train: 0.7828 acc_val: 0.8125\n",
      "Epoch: [938/1000] loss_train: 0.1543 acc_train: 0.8135 acc_val: 0.8145\n",
      "Epoch: [939/1000] loss_train: 0.1550 acc_train: 0.8053 acc_val: 0.8145\n",
      "Epoch: [940/1000] loss_train: 0.1568 acc_train: 0.8033 acc_val: 0.8145\n",
      "Epoch: [941/1000] loss_train: 0.1557 acc_train: 0.8156 acc_val: 0.8145\n",
      "Epoch: [942/1000] loss_train: 0.1577 acc_train: 0.7971 acc_val: 0.8145\n",
      "Epoch: [943/1000] loss_train: 0.1572 acc_train: 0.7848 acc_val: 0.8145\n",
      "Epoch: [944/1000] loss_train: 0.1571 acc_train: 0.7889 acc_val: 0.8164\n",
      "Epoch: [945/1000] loss_train: 0.1576 acc_train: 0.8156 acc_val: 0.8164\n",
      "Epoch: [946/1000] loss_train: 0.1646 acc_train: 0.7684 acc_val: 0.8164\n",
      "Epoch: [947/1000] loss_train: 0.1544 acc_train: 0.8115 acc_val: 0.8164\n",
      "Epoch: [948/1000] loss_train: 0.1569 acc_train: 0.7910 acc_val: 0.8164\n",
      "Epoch: [949/1000] loss_train: 0.1634 acc_train: 0.7746 acc_val: 0.8164\n",
      "Epoch: [950/1000] loss_train: 0.1588 acc_train: 0.7951 acc_val: 0.8164\n",
      "Epoch: [951/1000] loss_train: 0.1579 acc_train: 0.7930 acc_val: 0.8164\n",
      "Epoch: [952/1000] loss_train: 0.1545 acc_train: 0.8074 acc_val: 0.8164\n",
      "Epoch: [953/1000] loss_train: 0.1582 acc_train: 0.7910 acc_val: 0.8164\n",
      "Epoch: [954/1000] loss_train: 0.1597 acc_train: 0.7807 acc_val: 0.8184\n",
      "Epoch: [955/1000] loss_train: 0.1583 acc_train: 0.7848 acc_val: 0.8184\n",
      "Epoch: [956/1000] loss_train: 0.1594 acc_train: 0.7971 acc_val: 0.8184\n",
      "Epoch: [957/1000] loss_train: 0.1610 acc_train: 0.7910 acc_val: 0.8184\n",
      "Epoch: [958/1000] loss_train: 0.1560 acc_train: 0.7910 acc_val: 0.8184\n",
      "Epoch: [959/1000] loss_train: 0.1613 acc_train: 0.7664 acc_val: 0.8184\n",
      "Epoch: [960/1000] loss_train: 0.1590 acc_train: 0.7643 acc_val: 0.8184\n",
      "Epoch: [961/1000] loss_train: 0.1636 acc_train: 0.7684 acc_val: 0.8184\n",
      "Epoch: [962/1000] loss_train: 0.1579 acc_train: 0.7869 acc_val: 0.8184\n",
      "Epoch: [963/1000] loss_train: 0.1554 acc_train: 0.7930 acc_val: 0.8164\n",
      "Epoch: [964/1000] loss_train: 0.1580 acc_train: 0.7951 acc_val: 0.8164\n",
      "Epoch: [965/1000] loss_train: 0.1571 acc_train: 0.7807 acc_val: 0.8164\n",
      "Epoch: [966/1000] loss_train: 0.1572 acc_train: 0.7807 acc_val: 0.8164\n",
      "Epoch: [967/1000] loss_train: 0.1538 acc_train: 0.7992 acc_val: 0.8184\n",
      "Epoch: [968/1000] loss_train: 0.1602 acc_train: 0.7910 acc_val: 0.8184\n",
      "Epoch: [969/1000] loss_train: 0.1530 acc_train: 0.8053 acc_val: 0.8184\n",
      "Epoch: [970/1000] loss_train: 0.1600 acc_train: 0.7971 acc_val: 0.8184\n",
      "Epoch: [971/1000] loss_train: 0.1562 acc_train: 0.8012 acc_val: 0.8184\n",
      "Epoch: [972/1000] loss_train: 0.1571 acc_train: 0.8033 acc_val: 0.8184\n",
      "Epoch: [973/1000] loss_train: 0.1549 acc_train: 0.7992 acc_val: 0.8184\n",
      "Epoch: [974/1000] loss_train: 0.1599 acc_train: 0.7869 acc_val: 0.8184\n",
      "Epoch: [975/1000] loss_train: 0.1571 acc_train: 0.7889 acc_val: 0.8184\n",
      "Epoch: [976/1000] loss_train: 0.1570 acc_train: 0.8033 acc_val: 0.8164\n",
      "Epoch: [977/1000] loss_train: 0.1586 acc_train: 0.7828 acc_val: 0.8164\n",
      "Epoch: [978/1000] loss_train: 0.1552 acc_train: 0.8012 acc_val: 0.8164\n",
      "Epoch: [979/1000] loss_train: 0.1553 acc_train: 0.8135 acc_val: 0.8164\n",
      "Epoch: [980/1000] loss_train: 0.1623 acc_train: 0.7766 acc_val: 0.8164\n",
      "Epoch: [981/1000] loss_train: 0.1575 acc_train: 0.7807 acc_val: 0.8164\n",
      "Epoch: [982/1000] loss_train: 0.1593 acc_train: 0.7971 acc_val: 0.8164\n",
      "Epoch: [983/1000] loss_train: 0.1573 acc_train: 0.7684 acc_val: 0.8164\n",
      "Epoch: [984/1000] loss_train: 0.1615 acc_train: 0.7705 acc_val: 0.8164\n",
      "Epoch: [985/1000] loss_train: 0.1563 acc_train: 0.7930 acc_val: 0.8164\n",
      "Epoch: [986/1000] loss_train: 0.1606 acc_train: 0.7766 acc_val: 0.8164\n",
      "Epoch: [987/1000] loss_train: 0.1540 acc_train: 0.7889 acc_val: 0.8164\n",
      "Epoch: [988/1000] loss_train: 0.1546 acc_train: 0.8074 acc_val: 0.8164\n",
      "Epoch: [989/1000] loss_train: 0.1550 acc_train: 0.8176 acc_val: 0.8164\n",
      "Epoch: [990/1000] loss_train: 0.1610 acc_train: 0.7746 acc_val: 0.8164\n",
      "Epoch: [991/1000] loss_train: 0.1559 acc_train: 0.7951 acc_val: 0.8145\n",
      "Epoch: [992/1000] loss_train: 0.1579 acc_train: 0.7807 acc_val: 0.8145\n",
      "Epoch: [993/1000] loss_train: 0.1579 acc_train: 0.7766 acc_val: 0.8145\n",
      "Epoch: [994/1000] loss_train: 0.1567 acc_train: 0.8012 acc_val: 0.8145\n",
      "Epoch: [995/1000] loss_train: 0.1559 acc_train: 0.7889 acc_val: 0.8145\n",
      "Epoch: [996/1000] loss_train: 0.1596 acc_train: 0.7889 acc_val: 0.8145\n",
      "Epoch: [997/1000] loss_train: 0.1613 acc_train: 0.7869 acc_val: 0.8145\n",
      "Epoch: [998/1000] loss_train: 0.1558 acc_train: 0.7869 acc_val: 0.8145\n",
      "Epoch: [999/1000] loss_train: 0.1591 acc_train: 0.7828 acc_val: 0.8145\n",
      "Epoch: [1000/1000] loss_train: 0.1585 acc_train: 0.7725 acc_val: 0.8145\n",
      "Final accuracy is 0.8250\n"
     ]
    }
   ],
   "source": [
    "%run student_mvc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "recall: 0.8214\n",
      "acc_test: 0.7782\n",
      "recall: 0.7660\n",
      "acc_test: 0.7902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.42 s\n",
      "Wall time: 4.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%run eval_mvc.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "برای برنامه ریزی خطی با خطای زیر مواجه میشوم\n",
    "\n",
    "CPLEX Error  1016: Community Edition. Problem size limits exceeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# %run combhelper.py MVC LP BA5k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Problem: MVC\n",
      "GD:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time elapsed: 11.360s\n",
      "Solution size: 2823\n",
      "GD+COMBHelper_{pt}:\n",
      "Time elapsed: 3.217s\n",
      "Solution size: 2713\n",
      "Coverage: 0.9925\n",
      "GD+COMBHelper:\n",
      "Time elapsed: 3.534s\n",
      "Solution size: 2815\n",
      "Coverage: 0.9990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 18.3 s\n",
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%run combhelper.py MVC GD BA5k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Problem: MVC\n",
      "LS:\n",
      "Time elapsed: 97.857s\n",
      "Solution size: 2958\n",
      "LS+COMBHelper_{pt}:\n",
      "Time elapsed: 1.079s\n",
      "Solution size: 2702\n",
      "Coverage: 0.9925\n",
      "LS+COMBHelper:\n",
      "Time elapsed: 3.067s\n",
      "Solution size: 2802\n",
      "Coverage: 0.9990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 44s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%run combhelper.py MVC LS BA5k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally it worked :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
